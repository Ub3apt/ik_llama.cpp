<!DOCTYPE html>
<html lang="ru">

  <head>
    <title>Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к) &#x2F; Хабр</title>
<meta property="fb:app_id" content="444736788986613">
<meta property="fb:pages" content="472597926099084">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@habr_com">
<meta property="og:site_name" content="Хабр">
<link href="https://habr.com/ru/rss/post/921540/?fl=ru" type="application/rss+xml" title rel="alternate" name="rss">
<link href="https://habr.com/ru/articles/921540/" rel="canonical" data-hid="e3fa780">
<link rel="image_src" href="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png" data-hid="2a79c45">
<link rel="amphtml" href="https://habr.com/ru/amp/publications/921540/">
<meta property="og:title" content="Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)">
<meta name="twitter:title" content="Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)">
<meta name="aiturec:title" content="Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)">
<meta name="description" content="Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы...">
<meta itemprop="description" content="Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы...">
<meta property="og:description" content="Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы...">
<meta name="twitter:description" content="Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы...">
<meta property="aiturec:description" content="Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы...">
<meta itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png">
<meta property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png">
<meta name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png">
<meta property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png?format=vk">
<meta property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/21c/125/26f/21c12526f6b5a79b4350d686a03883e8.png?format=vk">
<meta property="aiturec:item_id" content="921540">
<meta property="aiturec:datetime" content="2025-06-29T08:46:58.000Z">
<meta content="https://habr.com/ru/articles/921540/" property="og:url">
<meta property="og:type" content="article">
<meta property="og:locale" content="ru_RU">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta name="keywords" content="llama.cpp, ik_llama, deepseek, локальные нейросети, deepseek r1, deepseek v3">
<script type="application/ld+json" data-hid="1e0f0a2">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/articles\/921540\/"},"headline":"Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)","datePublished":"2025-06-29T11:46:58+03:00","dateModified":"2025-06-30T12:36:28+03:00","author":{"@type":"Person","name":"Shannon"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, т...","url":"https:\/\/habr.com\/ru\/articles\/921540\/#post-content-body","about":["h_artificial_intelligence","f_popsci"],"image":["https:\/\/habr.com\/share\/publication\/921540\/4655f563d03dd272970ec33eb06ca4b4\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/37b\/790\/19b\/37b79019ba325f57f446bac66156b1e5.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/ac2\/b64\/ed0\/ac2b64ed0aaf4bf31a2fb9027d8003c5.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/a43\/e07\/23e\/a43e0723ec398948f69e9fb762c3c068.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/5ae\/612\/881\/5ae612881bf3c9c8321524e7b4ba5ce6.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/4fa\/63b\/483\/4fa63b48328fe5520c8d3e3bfcd2d762.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/50e\/ba9\/d1a\/50eba9d1a83031a1cf402184bafb1cf1.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/bb5\/2da\/fc0\/bb52dafc048b82e76591d95064ed899c.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/4b0\/7e3\/f18\/4b07e3f182c2fade910d5eb1ea8f547e.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/400\/ce3\/24f\/400ce324f0f32ea1672463ad415d2430.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/962\/efe\/edd\/962efeedd5c67dee42c5c6a2e46f6695.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/8a0\/d22\/34d\/8a0d2234d5015348adf430e72241917f.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/98e\/7fa\/0f1\/98e7fa0f16d46bbff8f5c35df40c34c2.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/5e9\/c5b\/92b\/5e9c5b92ba6f2597a01a97766de228ed.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/270\/e54\/5e0\/270e545e068f018fbf81d72b1dca4ea7.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/488\/f02\/787\/488f02787c0235bf8bb1d24d4a0dc810.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/ea8\/4ed\/39f\/ea84ed39faacf9cf33c20e575a09d127.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/2c0\/327\/d63\/2c0327d6306ad6e00e3501b6adea0baa.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/5de\/c67\/19b\/5dec6719bbf3e6f84797bda523430bd8.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/e79\/021\/938\/e7902193855bc67679ba2b46faf91439.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/7b2\/91b\/0f3\/7b291b0f377f40e1fdf0172eaf59e75d.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/461\/59b\/548\/46159b548ef07b417b78f0ad44b9c600.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/fde\/a2d\/b7a\/fdea2db7a50d436bbf2ef77e5af5eb75.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/bed\/402\/eb8\/bed402eb8dd4d567fd34377f9db28b88.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/b55\/bc9\/c75\/b55bc9c750990ad4ea0064428d09f53a.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/091\/0bc\/03c\/0910bc03ccb65ec1f3dbcae07bf5c761.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/426\/e5b\/144\/426e5b1443fa9a83a5fcdb7883504b66.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/803\/d8d\/dfb\/803d8ddfb7bcf5bc71659b95da363fa9.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/e88\/845\/ad9\/e88845ad9fb379e34bcca7c41548d68f.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/1dc\/a56\/00f\/1dca5600f818608b78d8224c7deb1ce3.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/cce\/df4\/cb7\/ccedf4cb7e786eb4a1506dc5656601be.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/939\/ece\/919\/939ece919421b269acfc548d0dfb01f9.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/ddf\/cb4\/d49\/ddfcb4d49e5ed7739e5f21853eaddcb2.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/1e3\/18a\/f12\/1e318af1282163f372005e30f02d1129.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/f04\/3f9\/613\/f043f9613dcd813280abd4659e775567.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/f11\/a69\/866\/f11a69866359b2e515288c5ea9fa82ae.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/ee5\/244\/f88\/ee5244f88aa2731992a971c84f56f810.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/9d3\/f40\/61c\/9d3f4061ca797cb9a7c5eb5b79542d36.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/b14\/5ac\/07d\/b145ac07d81439bbef2818d9ebd04170.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/78d\/8b0\/286\/78d8b0286ebb21cc5a7b6baf2926d74c.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/2e9\/ad1\/877\/2e9ad187775e5730da04c60190d3f448.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/4fe\/5a4\/910\/4fe5a491096df8cd4699d67b05a8b473.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/2c2\/6f4\/b2f\/2c26f4b2f83a2b97acae6e7738139707.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/4e2\/ac1\/0a3\/4e2ac10a3313f4cf0a4a682b2cc3af90.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/a96\/c2a\/1c3\/a96c2a1c3bdeb8cb87bb0754ba952f60.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/941\/4dc\/f66\/9414dcf66af59bff7b7f5b8071921e9b.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/589\/306\/148\/589306148d255c0eead9849fbc0230ff.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/21c\/125\/26f\/21c12526f6b5a79b4350d686a03883e8.png"]}</script>
    <meta charset="UTF-8">
    <meta name="viewport"
      content="width=device-width,initial-scale=1.0,viewport-fit=cover,maximum-scale=1,user-scalable=0">
    <meta name="referrer" content="unsafe-url">
    <style>
      /* cyrillic-ext */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9E4kDNxMZdWfMOD5VvmojLazX3dGTP.woff2) format('woff2');
        unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
      }

      /* cyrillic */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9E4kDNxMZdWfMOD5Vvk4jLazX3dGTP.woff2) format('woff2');
        unicode-range: U+0301, U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
      }

      /* latin-ext */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9E4kDNxMZdWfMOD5VvmYjLazX3dGTP.woff2) format('woff2');
        unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20C0, U+2113, U+2C60-2C7F, U+A720-A7FF;
      }

      /* latin */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9E4kDNxMZdWfMOD5Vvl4jLazX3dA.woff2) format('woff2');
        unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
      }

      /* cyrillic-ext */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 500;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnZKveSxf6Xl7Gl3LX.woff2) format('woff2');
        unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
      }

      /* cyrillic */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 500;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnZKveQhf6Xl7Gl3LX.woff2) format('woff2');
        unicode-range: U+0301, U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
      }

      /* latin-ext */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 500;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnZKveSBf6Xl7Gl3LX.woff2) format('woff2');
        unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20C0, U+2113, U+2C60-2C7F, U+A720-A7FF;
      }

      /* latin */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 500;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnZKveRhf6Xl7Glw.woff2) format('woff2');
        unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
      }

      /* cyrillic-ext */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 700;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnLK3eSxf6Xl7Gl3LX.woff2) format('woff2');
        unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
      }

      /* cyrillic */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 700;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnLK3eQhf6Xl7Gl3LX.woff2) format('woff2');
        unicode-range: U+0301, U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
      }

      /* latin-ext */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 700;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnLK3eSBf6Xl7Gl3LX.woff2) format('woff2');
        unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20C0, U+2113, U+2C60-2C7F, U+A720-A7FF;
      }

      /* latin */
      @font-face {
        font-family: 'Fira Sans';
        font-style: normal;
        font-weight: 700;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/firasans/v17/va9B4kDNxMZdWfMOD5VnLK3eRhf6Xl7Glw.woff2) format('woff2');
        unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
      }
    </style>
    <link rel="preload" href="https://assets.habr.com/habr-web/css/theme/light-v1.css" as="style" media="(prefers-color-scheme: light)" /><link rel="preload" href="https://assets.habr.com/habr-web/css/theme/dark-v1.css" as="style" media="(prefers-color-scheme: dark)" /><link id="light-colors" rel="stylesheet" href="https://assets.habr.com/habr-web/css/theme/light-v1.css" media="(prefers-color-scheme: light)" /><link id="dark-colors" rel="stylesheet" href="https://assets.habr.com/habr-web/css/theme/dark-v1.css" media="(prefers-color-scheme: dark)" />
    <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.c04c4b5d2b029634fa11ff01d6ac1907.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
    <style>
      .grecaptcha-badge {
        visibility: hidden;
      }
    </style>
    <meta name="habr-version" content="2.248.1">
    
    <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
    <meta name="msapplication-TileColor" content="#629FBC">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="mobile-web-app-capable" content="yes">
    <link rel="shortcut icon" type="image/png" sizes="16x16" href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png">
    <link rel="shortcut icon" type="image/png" sizes="32x32" href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png">
    <link rel="apple-touch-icon" type="image/png" sizes="152x152" href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png">
    <link rel="apple-touch-icon" type="image/png" sizes="256x256" href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png">
    <link rel="apple-touch-startup-image"
      media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
      href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png">
    <link rel="mask-icon" color="#77a2b6" href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg">
    <link crossorigin="use-credentials" href="/manifest.webmanifest" rel="manifest">
    <script async src="https://unpkg.com/pwacompat" crossorigin="anonymous"></script>
    <script>window.yaContextCb = window.yaContextCb || []</script>
    <script src="https://yandex.ru/ads/system/context.js" async></script>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.04465f7c.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.7ab9e5e2.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.e0d6a80f.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.8728a42c.js" as="script"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.04465f7c.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.e0d6a80f.css"></head>
  <body>
    
    <div id="mount"><div id="app"><div class="tm-layout__wrapper"><!--[--><!----><div></div><!----><header class="tm-header" data-test-id="header"><div class="tm-page-width"><!--[--><div class="tm-header__container"><!----><span class="tm-header__logo-wrap"><a class="tm-header__logo tm-header__logo_hl-ru tm-header__logo" href="/ru/"><svg class="tm-svg-img tm-header__icon" height="16" width="16"><title>Хабр</title><use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a><span style="display:none;" class="tm-header__beta-sign">β</span></span><!--[--><div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><!--[--><button class="tm-header__dropdown-toggle"><svg class="tm-svg-img tm-header__icon tm-header__icon_dropdown" height="16" width="16"><title>Открыть список</title><use xlink:href="/img/megazord-v28.7909a852..svg#arrow-down"></use></svg></button><!--]--></div><!----></div><a href="/ru/sandbox/start/" class="tm-header__become-author-btn">Как стать автором</a><!----><!----><!--]--><!----></div><!--]--></div></header><div class="tm-layout"><div class="tm-page-progress-bar"></div><div class="tm-base-layout__header_is-sticky tm-base-layout__header" data-menu-sticky="true"><div class="tm-page-width"><!--[--><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/feed/">Моя лента</a><!--]--><!--[--><a class="tm-main-menu__item" href="/ru/articles/">Все потоки</a><!--]--><!--[--><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/flows/develop/">Разработка</a><!--]--><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/flows/admin/">Администрирование</a><!--]--><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/flows/design/">Дизайн</a><!--]--><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/flows/management/">Менеджмент</a><!--]--><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/flows/marketing/">Маркетинг</a><!--]--><!--[--><a class="tm-main-menu__item" data-test-id="main-menu-item" href="/ru/flows/popsci/">Научпоп</a><!--]--><!--]--></nav></div></div><div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search" data-test-id="search-button"><svg class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark" height="24" width="24"><title>Поиск</title><use xlink:href="/img/megazord-v28.7909a852..svg#search"></use></svg></a><!----><!----><div class="tm-header-user-menu__item tm-header-user-menu__write"><div><svg class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_write tm-header-user-menu__icon_dark" height="24" width="24"><title>Написать публикацию</title><use xlink:href="/img/megazord-v28.7909a852..svg#write"></use></svg></div><!----></div><!--[--><div class="tm-header-user-menu__item"><button class="tm-header-user-menu__toggle" data-test-id="user-menu-settings"><svg class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_dark" height="24" width="24"><title>Настройки</title><use xlink:href="/img/megazord-v28.7909a852..svg#page-settings"></use></svg></button></div><a href="https://habr.com/kek/v1/auth/habrahabr/?back=/ru/articles/921540/&amp;hl=ru" rel="nofollow" class="tm-header-user-menu__item"><!--[--><button class="btn btn_solid btn_small tm-header-user-menu__login" type="button"><!--[-->Войти<!--]--></button><!--]--></a><!--]--><!----><!--teleport start--><!--teleport end--><!----></div></div><!--]--></div></div><!----><div class="tm-page-width"><!--[--><!--]--></div><main class="tm-layout__container"><div class="tm-page" hl="ru" data-async-called="true" style="--244a8968:16px;--72d26ab6:100%;--4e17cdfb:0;"><div class="tm-page-width"><!--[--><!----><div class="tm-page__wrapper"><div class="tm-page__main_has-sidebar tm-page__main"><div class="pull-down"><!----><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg class="tm-svg-img pull-down__icon pull-down__arrow" height="24" width="24"><title>Обновить</title><use xlink:href="/img/megazord-v28.7909a852..svg#pull-arrow"></use></svg></div></div><!--[--><!--[--><!----><div class="tm-article-presenter"><!--[--><!--]--><div class="tm-article-presenter__body" data-test-id="article-body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><!--[--><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><!--[--><div class="tm-article-presenter__header"><!--[--><!--]--><div class="tm-article-snippet tm-article-snippet tm-article-presenter__snippet"><!--[--><!--]--><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/Shannon/" class="tm-user-info__userpic" data-test-id="user-info-pic" title="Shannon"><div class="tm-entity-image"><!--[--><img alt="" class="tm-entity-image__pic" height="24" src="https://assets.habr.com/habr-web/img/avatars/126.png" width="24"><!--]--></div></a><span class="tm-user-info__user tm-user-info__user_appearance-default" data-test-id="user-info-description"><a href="/ru/users/Shannon/" class="tm-user-info__username">Shannon <!----></a><!--[--><span class="tm-article-datetime-published"><time datetime="2025-06-29T08:46:58.000Z" title="2025-06-29, 08:46">вчера в 08:46</time></span><!--]--></span></span></div><!----></div><h1 class="tm-title tm-title_h1" lang="ru" data-test-id="articleTitle"><span>Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)</span></h1><div class="tm-article-snippet__stats" data-test-id="articleStats"><!----><div class="tm-article-reading-time"><span class="tm-svg-icon__wrapper tm-article-reading-time__icon"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Время на прочтение</title><use xlink:href="/img/megazord-v28.7909a852..svg#clock"></use></svg></span><span class="tm-article-reading-time__label">20 мин</span></div><span class="tm-icon-counter tm-data-icons__item"><svg class="tm-svg-img tm-icon-counter__icon" height="24" width="24"><title>Количество просмотров</title><use xlink:href="/img/megazord-v28.7909a852..svg#counter-views"></use></svg><span class="tm-icon-counter__value" title="20965">21K</span></span></div><div class="tm-publication-hubs__container" data-test-id="articleHubsList"><div class="tm-publication-hubs"><!--[--><span class="tm-publication-hub__link-container"><a href="/ru/hubs/artificial_intelligence/" class="tm-publication-hub__link"><!--[--><span>Искусственный интеллект</span><!----><!--]--></a></span><!--]--></div></div><div class="tm-article-labels" data-test-id="articleLabels" data-v-b9df236e><div class="tm-article-labels__container" data-v-b9df236e><!----><!--[--><div class="tm-publication-label tm-publication-label_variant-tutorial" data-v-b9df236e><span>Туториал</span></div><!--[--><!--]--><!--]--></div></div><!----><!----></div></div><!--[--><!----><div class="tm-article-body" data-gallery-root lang="ru"><div><!--[--><!--]--></div><div id="post-content-body"><div><div class="article-formatted-body article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/37b/790/19b/37b79019ba325f57f446bac66156b1e5.png" width="1139" height="650" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/37b/790/19b/37b79019ba325f57f446bac66156b1e5.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/37b/790/19b/37b79019ba325f57f446bac66156b1e5.png 781w" loading="lazy" decode="async"/></figure><p>Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы она обучалась в стандартных f16, её вес был бы 1400гб, а мы попробуем версию в 10 раз меньше. Запустим самый маленький 1.66-битный IQ1_S_R4 квант полноценной модели размером 130гб на игровом ПК, отдельно с 4090 и 4060ti. Загрузим туда очень-очень много контекста и проверим, такой квант всё ещё способен давать разумные ответы или нет.</p><h2>Как запускать</h2><p>Почти все массовые локальные модели запускаются одинаковым образом, есть движок llama.cpp и формат этого движка gguf, с различными вариантами квантования, и есть оболочки, которые под капотом запускают тот самый llama.cpp - это и ollama, и LM Studio и все остальные.</p><p>Чтобы просто запустить локально любую небольшую модель, достаточно скачать <a href="https://lmstudio.ai/" rel="noopener noreferrer nofollow">LM Studio</a>, <a href="https://jan.ai/" rel="noopener noreferrer nofollow">Jan</a> или, если нужен более гибкий функционал, <a href="https://github.com/oobabooga/text-generation-webui" rel="noopener noreferrer nofollow">text-generation-webui</a>, потом любым способом скачать gguf файл и запустить всё в пару кликов. Это будет работать локально и займет несколько минут на разобраться. Всё это работает и на nvidia, и на amd, и на intel, там где нет CUDA, отлично работает через Vulkan. </p><p>Но сегодня нас интересует кое-что посложнее, запустить настоящую большую DeepSeek R1-0528 размером 671B на домашнем игровом ПК. Это запуск не на б/у сервере, не на каком-то специфичном дорогом железе, не на куче видеопамяти, а на обычном ПК.</p><p>Запускать будем не обычное квантование вроде Q4_K_M или IQ1_S, и не динамическое квантование UD-...-XL, которое превосходит обычные кванты. Нас интересует sota квантование iq4_ks и R4, которое работает только в ik_llama.</p><p><a href="https://github.com/ikawrakow/ik_llama.cpp" rel="noopener noreferrer nofollow">ik_llama.cpp</a> - это форк от llama.cpp, который улучшает производительно на CPU и имеет расширенную поддержку MoE моделей, а так же является создателем передовых новых квантов. Именно через iq4_ks и R4 стало возможно создать настолько маленький квант, который ещё может показывает адекватные результаты и влезает в домашний ПК.</p><h3>На чём запускать</h3><p>Нам нужно много памяти, минимум можно попробовать 128гб, сейчас комплект из 4х модулей памяти 48гб DDR5 для домашних ПК стоит в пределах 50к, DDR4 4x32гб в 2 раза дешевле, и в продаже также начали появляться недорогие модули 2x64гб. Можно сказать, что 128/192гб ram это уже вполне доступное железо.</p><p>Сами характеристики ПК не так важны, если там есть 6-8 ядер, важнее объем памяти и наличие 1 GPU, что является ключевым фактором для ускорения. Когда используются именно 4 модуля по 48гб, они плохо держат разгон и не стартуют на XMP, но хватит и того, что они запускаются на базовой частоте 4800.</p><p>Характеристики испытуемого ПК: </p><ul><li><p>CPU: i7-14700</p></li><li><p>Материнка: GIGABYTE Z790 D AX</p></li><li><p>ОЗУ: 4x 48gb Kingbank DDR5 4800 MT/s</p></li><li><p>GPU: 4060 Ti 16gb, 4090 24gb</p></li></ul><p>Сравним отдельно 4090 и 4060 ti (хотя сейчас уже актуальнее 5060 Ti 16гб, у неё в 1.5 раза быстрее память чем у 4060ti, а стоит столько же), чтобы понять влияние GPU на скорость. </p><h2>Что запускать? Какой квант?</h2><h4>DeepSeek-R1-0528-IQ1_S_R4 и DeepSeek-V3-0324-IQ1_S_R4</h4><p>Нас интересует репозиторий <a href="https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/" rel="noopener noreferrer nofollow">ubergarm/DeepSeek-R1-0528-GGUF</a> - пока единственный кто предоставляет готовые кванты для ik_llama. Среди них нам нужен самый маленький размером 130гб - это IQ1_S_R4. </p><p>Если не хотите ждать долгих рассуждений от R1, то можно взять V3-0324, для него тоже есть такой квант: <a href="https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF/tree/main/DeepSeek-V3-0324-IQ1_S_R4" rel="noopener noreferrer nofollow">DeepSeek-V3-0324-IQ1_S_R4</a></p><p>Квант экстремально малого размера, и замеры качества через PPL показывают, что он ощутимо отстает от оригинала.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/ac2/b64/ed0/ac2b64ed0aaf4bf31a2fb9027d8003c5.png" alt="Чем ниже PPL, тем лучше" title="Чем ниже PPL, тем лучше" width="780" height="256" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/ac2/b64/ed0/ac2b64ed0aaf4bf31a2fb9027d8003c5.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/ac2/b64/ed0/ac2b64ed0aaf4bf31a2fb9027d8003c5.png 781w" loading="lazy" decode="async"/><div><figcaption>Чем ниже PPL, тем лучше</figcaption></div></figure><p>Но надежда есть, так как PPL не лучший показатель, он не отображает реальное качество кванта, и замеры KLD намного лучше отображают как квант далек от оригинала.</p><p>Вот сравнение 3 малых квантов: от Bartowski, Unsloth и Ubergarm, каждый со своей версией минимального размера. Все показатели чем ниже, тем лучше. Квант R4 имея самый маленький размер, показывает что обладает каким-то качеством:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/a43/e07/23e/a43e0723ec398948f69e9fb762c3c068.png" alt="KLD отклонения от Q8_0, чем ниже, тем лучше" title="KLD отклонения от Q8_0, чем ниже, тем лучше" width="1782" height="1160" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/a43/e07/23e/a43e0723ec398948f69e9fb762c3c068.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/a43/e07/23e/a43e0723ec398948f69e9fb762c3c068.png 781w" loading="lazy" decode="async"/><div><figcaption>KLD отклонения от Q8_0, чем ниже, тем лучше</figcaption></div></figure><details class="spoiler"><summary>Про разницу PPL и KLD</summary><div class="spoiler__content"><p>В работе Accuracy is Not All You Need (<a href="https://arxiv.org/abs/2407.09141" rel="noopener noreferrer nofollow">https://arxiv.org/abs/2407.09141</a>) показали, что KLD лучше отображает корреляцию между ошибками квантования и метрикой KLD, чем PPL, так как PPL скрывает ошибки квантования из-за усреднения.</p><p>PPL (Perplexity) - это степень неуверенности модели в предсказании токена, чем ниже, тем увереннее модель. PPL усредняет логарифмические вероятности по всем токенам, поэтому ошибки, например, завышение вероятности одних токенов и занижение других, могут компенсировать друг друга - в результате PPL близок к оригиналу, хотя результат искажен. Ещё PPL слабо реагирует на ошибки в редких токенах, важных для генерации разнообразных ответов.</p><p>KLD (KL Divergence) измеряет расхождение между распределениями исходной и квантованной моделей для каждого токена, потом суммирует расхождения для всех токенов. Тут ошибки никак не компенсируются друг другом, отклонения в вероятностях редких и частых токенов одинаково повлияют на итог. Это куда лучше позволяет оценить потери при квантовании, и если оптимизировать квантование под минимизацию KLD, то в среднем это улучшает кванты.</p></div></details><h2>Замеры скорости памяти</h2><p>Скорость памяти прямо пропорциональна скорости генерации.</p><p>2 модуля памяти, даже модули 48гб, обычно хорошо разгоняются и держать XMP 6400, на такой частоте можно получить почти 100гб/с. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/5ae/612/881/5ae612881bf3c9c8321524e7b4ba5ce6.png" width="539" height="518" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/5ae/612/881/5ae612881bf3c9c8321524e7b4ba5ce6.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/5ae/612/881/5ae612881bf3c9c8321524e7b4ba5ce6.png 781w" loading="lazy" decode="async"/></figure><p>Но так как нам нужно больше памяти, то замерим скорости работы на 4х модулях. Скорость чтения на 4x DDR5-4800 равна 70 Гб/с, это не очень быстро, это ближе к скорости хорошей DDR4, чем к DDR5-6400, но этого должно хватить.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/4fa/63b/483/4fa63b48328fe5520c8d3e3bfcd2d762.png" width="539" height="518" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/4fa/63b/483/4fa63b48328fe5520c8d3e3bfcd2d762.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/4fa/63b/483/4fa63b48328fe5520c8d3e3bfcd2d762.png 781w" loading="lazy" decode="async"/></figure><p>Проверим скорость Gemma3, генерация только на CPU, в стандартном кванте Q4_K_M. Запуск обычной llama.cpp на Windows 10, планировщик не оптимизирован на работу с малыми и большими ядрами:</p><p><code>.\llama-bench -m "gemma-3-12b-it-Q4_K_M.gguf" -t 4 -t 6 -t 8 -t 20 -t 28</code></p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/50e/ba9/d1a/50eba9d1a83031a1cf402184bafb1cf1.png" alt="Gemma3 12B Q4_K_M" title="Gemma3 12B Q4_K_M" width="947" height="545" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/50e/ba9/d1a/50eba9d1a83031a1cf402184bafb1cf1.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/50e/ba9/d1a/50eba9d1a83031a1cf402184bafb1cf1.png 781w" loading="lazy" decode="async"/><div><figcaption>Gemma3 12B Q4_K_M</figcaption></div></figure><p><code>.\llama-bench -m "gemma-3-27b-it-Q4_K_M.gguf" -t 4 -t 6 -t 8 -t 20 -t 28</code></p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/bb5/2da/fc0/bb52dafc048b82e76591d95064ed899c.png" alt="Gemma3 27B Q4_K_M" title="Gemma3 27B Q4_K_M" width="942" height="573" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/bb5/2da/fc0/bb52dafc048b82e76591d95064ed899c.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/bb5/2da/fc0/bb52dafc048b82e76591d95064ed899c.png 781w" loading="lazy" decode="async"/><div><figcaption>Gemma3 27B Q4_K_M</figcaption></div></figure><p>pp - это promp processing, он же prefill. В pp входит системный промпт и вся история диалога. До тех пор пока контекст не закэширован, всё будет считаться от самого начала. </p><p>tg - это token generation, генерация новых токенов, обычно все обращают внимание только на этот показатель, но на огромном контексте pp будет так же важен.</p><h2>Как запускать R1 671B на одной GPU и за счёт чего ускорение</h2><p>Память не очень быстрая, даже Gemma3 12B весом 7гб еле выходит за границу комфортности, которая составляет 5 t/s. В таких условия нам нужно запустить квант весом 130гб имея для ускорения всего 1 GPU и на сколько это вообще возможно.</p><p>Gemma3 это dense-модель, то есть сплошная, для каждого нового токена нужно обойти все параметры модели. DeepSeek V3/R1 - это MoE модель, где на каждом шагу использует только часть параметров.</p><p>MoE это сокращение архитектуры Mixture of Experts, в таких моделях количество параметров (B) всей модели больше чем количество активных параметров (AxB) необходимых для каждого нового токена. Например, модель Qwen3-235B-A22B имеет всего 235B параметров и  на каждом шагу из них только 22B будут активными.</p><p>У R1 количество параметров 671B, активных параметров 37B. Всего 61 слой, 3 слоя общих, которые используются на каждом шагу, остальные слои экспертов, они выбираются роутером на каждом шагу разные, поэтому нельзя просто загрузить 37B в vram.</p><p>Если модель целиком влезает в память, то скорость инференса будет примерно равна dense-модели размером 37B, а именно в районе 2 t/s. Это генерация на такой скорости памяти, была бы память быстрее, то и генерация была бы быстрее. 2 t/s мало, нужна помощь от GPU, но если просто выгрузить 10 слоев в vram, то будет ситуация, когда только первые 3 слоя полезны, остальные будут выпадать лишь иногда.</p><p>Выгрузив часть слоев ускорение есть, но совсем не существенное, нужно больше:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/4b0/7e3/f18/4b07e3f182c2fade910d5eb1ea8f547e.png" alt="" title="" width="645" height="100" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/4b0/7e3/f18/4b07e3f182c2fade910d5eb1ea8f547e.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/4b0/7e3/f18/4b07e3f182c2fade910d5eb1ea8f547e.png 781w" loading="lazy" decode="async"/></figure><p>Решение: <em>override-tensor</em>.</p><p>Чтобы получить существенное ускорение, нужно чтобы GPU полноценно участвовала в работе на каждом шагу и решение тут в том, чтобы на GPU выгрузить только веса внимания, которые легкие и на каждом шагу важны. А экспертные ffn каждого слоя оставить на CPU. </p><p>В llm трансформерах модель состоит из слоев, каждый слой состоит из 2х видов тензоров: внимания (attn, attention) и полносвязной сети (ffn, feed forward network). Посмотреть структуру модели можно на huggingface если нажать на конкретный квант. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/400/ce3/24f/400ce324f0f32ea1672463ad415d2430.png" alt="18-й слой deepseek r1, самые тяжелые тензоры это ffn, а важные attn намного легче" title="18-й слой deepseek r1, самые тяжелые тензоры это ffn, а важные attn намного легче" width="692" height="475" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/400/ce3/24f/400ce324f0f32ea1672463ad415d2430.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/400/ce3/24f/400ce324f0f32ea1672463ad415d2430.png 781w" loading="lazy" decode="async"/><div><figcaption>18-й слой deepseek r1, самые тяжелые тензоры это ffn, а важные attn намного легче</figcaption></div></figure><p>Тензоры внимания весят не очень много, и в таком низком кванте они влезают даже в 10гб видеопамяти, оставляя достаточно места для контекста, а ffn экспертов и составляют основной объем модели.</p><h2>--override-tensor или -ot</h2><p>Для того, чтобы разнести разбить слой на тензоры в llama.cpp и ik_llama добавили параметр <code>-ot</code> или <code>--override-tensors</code> , через него указывают какие тензоры отправятся на CPU (или другие устройства, например, вторую GPU) используя regexp синтаксис.</p><p>Нужно выгрузить все легковесные тензоры на GPU, а тяжелые, которые упрощенно называют MoE-параметрами, на CPU. Чтобы это сделать, нужно сначала выгрузить все слои на видеокарту через параметр <code>-ngl 999</code> , а потом указываем какие надо перенаправить в обычную память.</p><p>MoE-параметры это те, которые имеют в имени exps, то есть эксперты, поэтому нужно просто указать один из синонимов, который выберет всех exps:</p><p><code>-ot exps=CPU</code></p><p><code>-ot ".ffn_.*_exps.=CPU"</code> </p><p><code>-ot "([0-9]+).ffn_.*_exps.=CPU"</code> </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/962/efe/edd/962efeedd5c67dee42c5c6a2e46f6695.png" alt="Точные имена тензоров, чтобы составить правильный regexp" title="Точные имена тензоров, чтобы составить правильный regexp" width="702" height="478" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/962/efe/edd/962efeedd5c67dee42c5c6a2e46f6695.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/962/efe/edd/962efeedd5c67dee42c5c6a2e46f6695.png 781w" loading="lazy" decode="async"/><div><figcaption>Точные имена тензоров, чтобы составить правильный regexp</figcaption></div></figure><p>Теперь на видеокарте остались какие-то тензоры каждого слоя, и gpu на каждом шаге будет участвовать в генерации, за счет этого и происходит ускорение. И это в основном подходит только для MoE моделей.</p><p>И, обычно, если осталась свободная VRAM, или не нужно так много контекста, или есть вторая видеокарта, то можно больше тензоров оставить выгруженными на видеокарте. </p><p> <code>-ot "blk\.([4-9])\.ffn.*=CUDA0" -ot "blk\.(1[0-5])\.ffn.*=CUDA1" -ot exps=CPU</code>  </p><p>Так мы оставим 4-9 слои целиком на 1 GPU и 10-15 слои на 2 GPU. Для rocm или vulkan будут свои синонимы названия устройств, вроде Vulkan0 вместо CUDA0.</p><p>В кванте IQ1_S_R4 для тензоров ffn up|gate|down использованы тензоры R4, которые оптимизированы для работы на CPU, их нужно автоматически конвертировать в IQK квант пригодные для GPU. Для этого ik_llama надо скомпилировать с флагом <code><em>-DGGML_CUDA_IQK_FORCE_BF16=1</em></code></p><h2>Запускаем DeepSeek R1 671B IQ1_S_R4</h2><p>У <a href="https://github.com/ikawrakow/ik_llama.cpp" rel="noopener noreferrer nofollow">ik_llama</a> нет готовых бинарников, как у <a href="https://github.com/ggml-org/llama.cpp/releases" rel="noopener noreferrer nofollow">llama.cpp</a>, поэтому нужно будет собрать её из исходников, делается это по той же инструкции как и llama.cpp, поэтому сложности не должно возникнуть. </p><p>Если одна GPU. Вместо <code>-j28</code> укажите ваше количество ядер или потоков:</p><pre><code class="bash">git clone https://github.com/ikawrakow/ik_llama.cpp
cd ik_llama
cmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF
cmake --build build --config Release -j28
cd build/bin</code></pre><p>Если планируется использовать несколько GPU:</p><pre><code class="bash">git clone https://github.com/ikawrakow/ik_llama.cpp
cd ik_llama
cmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF -DGGML_SCHED_MAX_COPIES=1 -DGGML_CUDA_IQK_FORCE_BF16=1
cmake --build build --config Release -j28
cd build/bin</code></pre><p>Если не указать <code>-DGGML_SCHED_MAX_COPIES=1</code>, то будет перерасход видеопамяти с использованием <code>-ot</code>. <code>-DGGML_CUDA_IQK_FORCE_BF16=1</code> нужен для выгрузки на GPU ffn тензоров, но не всегда дает ускорение, иногда замедляет работу, зависит от модели видеокарты.</p><p>После этого можно запустить llama-server:</p><p><code>./llama-server -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -ctk q8_0 -amb 512 -fmoe -ot exps=CPU -ngl 99 -b 4096 -ub 4096 -t 20 -c 8192 </code></p><p>По адресу <a href="http://127.0.0.1:8080/" rel="noopener noreferrer nofollow">http://127.0.0.1:8080/</a> будет доступен вполне удобный веб-клиент:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a0/d22/34d/8a0d2234d5015348adf430e72241917f.png" alt="2184 токенов размышления, модель долго искала подвох, ответ 87 токенов. Скорость 7 t/s" title="2184 токенов размышления, модель долго искала подвох, ответ 87 токенов. Скорость 7 t/s" width="1068" height="511" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/8a0/d22/34d/8a0d2234d5015348adf430e72241917f.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a0/d22/34d/8a0d2234d5015348adf430e72241917f.png 781w" loading="lazy" decode="async"/><div><figcaption>2184 токенов размышления, модель долго искала подвох, ответ 87 токенов. Скорость 7 t/s</figcaption></div></figure><p>Мы получили 7 t/s, это в 2 раза выше, чем мы получали когда вынесли только начальные слои на GPU, хотя в обоих случая количество занимаемой памяти подобрано одинаково, что показывает, что подход через <code>-ot</code> работает.</p><p>Подробнее про параметры запуска:</p><p><code>-m</code> - путь до файла модели, у huggingface есть ограничение на размер файла 50гб, поэтому файлов будет несколько разбитых по шаблону 00001-0000x.gguf. Для запуска нужно указывать только 1 файл.</p><p><code>-fa</code> - flash attention, способ эффективнее считать контекст, тратя меньше памяти, при правильной реализации математически идентичен обычному attention.</p><p><code>-amb 512</code> - переиспользовать <a href="https://github.com/ikawrakow/ik_llama.cpp/pull/237" rel="noopener noreferrer nofollow">буфер для вычисления K*Q</a>, размер в mb, можно увеличить, если хватает памяти.</p><p><code>-fmoe</code> - <a href="https://github.com/ikawrakow/ik_llama.cpp/pull/229" rel="noopener noreferrer nofollow">fused moe</a>, объединяет up, gate и act операции, немного ускоряя вычисления.</p><p><code>-mla 3</code> - включить mla</p><p><code>-ctk q8_0</code> - квантование только k-кэша контекста, считается, что он почти не страдает от квантования, в отличии от v-кэша.</p><p><code>-ngl 99</code> - выгрузить все слои на GPU.</p><p><code>-ot exps=CPU</code> - отправить все тензоры где в имени exps на CPU.</p><p><code>-b 4096 -ub 4096</code> - оптимизация размеров батчей, может ускорить вычисление pp.</p><p><code>-t 20</code> - использовать все ядра, что есть, по умолчанию 8.</p><p><code>-c 8192</code> - задать размер контекста 8к, по умолчанию 4к.</p><p>Дополнительные параметры:</p><p><code>-rtr</code> - если запуск CPU only, то при загрузке конвертировать веса в оптимизированные для работы на CPU, отключает mmap</p><p><code>-ser 6,1</code> - умное <a href="https://github.com/ikawrakow/ik_llama.cpp/pull/239" rel="noopener noreferrer nofollow">уменьшение количества экспертов</a>, ускоряет работу за счет небольшого снижения качества.</p><p><code>-ts 24,16</code> - если установлены две gpu, то можно распределить по ним слои в заданной пропорции, полезно для dense-моделей, если используется -ot, то лучше не использовать.</p><h2>Бенчмарк скорости DeepSeek R1</h2><p>Теперь можно заняться более точным измерением скорости генерации как новых токенов, так и уже существующего контекста. У ik_llama есть удобный инструмент для замера скоростей модели на указанной длине контекста. </p><p>Сравним 3 варианта, вначале на типичном контексте 4к, параметры -b и -ub по-умолчанию:</p><ul><li><p>CPU only (скорость памяти 72 гб/с)</p></li><li><p>Ускорение через 4060 ti 16гб (скорость памяти 288 гб/с)</p></li><li><p>Ускорение через 4090 24гб (скорость памяти 1008 гб/с)</p></li></ul><p>N_KV - размер контекста.</p><p>T_PP - время генерации PP.</p><p>S_PP - скорость генерации pp.</p><p>S_TG - скорость генерации tg.</p><p>CPU only, скрываем все CUDA устройства через <code>CUDA_VISIBLE_DEVICES=""</code>:</p><p><code>CUDA_VISIBLE_DEVICES="" ./llama-sweep-bench -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ctk q8_0 -t 28 -c 4096</code></p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/98e/7fa/0f1/98e7fa0f16d46bbff8f5c35df40c34c2.png" alt="" title="" width="934" height="498" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/98e/7fa/0f1/98e7fa0f16d46bbff8f5c35df40c34c2.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/98e/7fa/0f1/98e7fa0f16d46bbff8f5c35df40c34c2.png 781w" loading="lazy" decode="async"/></figure><p>4060 ti:</p><p><code>CUDA_VISIBLE_DEVICES="1" ./llama-sweep-bench -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 99 -ctk q8_0 -t 28 -c 4096</code><br/> </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/5e9/c5b/92b/5e9c5b92ba6f2597a01a97766de228ed.png" width="930" height="447" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/5e9/c5b/92b/5e9c5b92ba6f2597a01a97766de228ed.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/5e9/c5b/92b/5e9c5b92ba6f2597a01a97766de228ed.png 781w" loading="lazy" decode="async"/></figure><p>4090:</p><p><code>CUDA_VISIBLE_DEVICES="0" ./llama-sweep-bench -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 99 -ctk q8_0 -t 28 -c 4096</code></p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/270/e54/5e0/270e545e068f018fbf81d72b1dca4ea7.png" width="941" height="450" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/270/e54/5e0/270e545e068f018fbf81d72b1dca4ea7.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/270/e54/5e0/270e545e068f018fbf81d72b1dca4ea7.png 781w" loading="lazy" decode="async"/></figure><p>Разница между 4060 ti и 4090 не соответствует разнице производительности и скорости памяти. Всё потому, что на GPU уходит всего ~10 гб тензоров и при маленьком батче разница между видеокартами не так заметна.</p><p>Теперь замерим контекст 32к, увеличим размер батчей, параметры <code>-b 4096 -ub 4096</code>. Увеличивая размер батчей, мы увеличим скорость PP, что важно при работе с большим контекстом, когда обработка 100к может занимать почти час на 25 t/s.</p><p>4060 ti:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/488/f02/787/488f02787c0235bf8bb1d24d4a0dc810.png" width="932" height="451" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/488/f02/787/488f02787c0235bf8bb1d24d4a0dc810.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/488/f02/787/488f02787c0235bf8bb1d24d4a0dc810.png 781w" loading="lazy" decode="async"/></figure><p>4090: </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/ea8/4ed/39f/ea84ed39faacf9cf33c20e575a09d127.png" width="934" height="443" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/ea8/4ed/39f/ea84ed39faacf9cf33c20e575a09d127.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/ea8/4ed/39f/ea84ed39faacf9cf33c20e575a09d127.png 781w" loading="lazy" decode="async"/></figure><p>Тут разница на подготовку промпта между картами уже видна лучше, скорость памяти 1 Тб/с против 288 Гб/с - всё-таки разница большая. На скорости 1 Тб/с для такого небольшого объема данных уже и количество ядер может быть важным.</p><p>На 4090 скорость tg немного упала, это не критично, зато скорость pp выросла почти в 10 раз до 200-300 t/s. С такой скоростью уже можно обрабатывать огромные контексты. На 4060 ti pp вырос всего в 1.5-2 раза, это тоже не плохо, обработка 32к контекста займет 15 минут, а дальше она закэшируется и будет работать моментально. </p><p>Параметром <code>-ser 6,1</code> можно компенсировать потери tg от <code>-ub 4096 -b 4096</code>, сохраняя 300 t/s на pp, и на коротком контексте возвращая 8 t/s на tg:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/2c0/327/d63/2c0327d6306ad6e00e3501b6adea0baa.png" width="932" height="198" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/2c0/327/d63/2c0327d6306ad6e00e3501b6adea0baa.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/2c0/327/d63/2c0327d6306ad6e00e3501b6adea0baa.png 781w" loading="lazy" decode="async"/></figure><h2>Как вместить огромный контекст в одну GPU?</h2><p>Как получить ускорение через GPU разобрались. Теперь нужно разобраться как вместить огромный контекст в тот небольшой объем vram, который остаётся.</p><p>У DeepSeek R1 максимальный контекст 160к, это очень много, и в обычном виде на это требуется сотни гб памяти. Например, согласно исследованию <a href="https://arxiv.org/abs/2505.02390" rel="noopener noreferrer nofollow">Quantitative Analysis of Performance Drop in DeepSeek Model Quantization</a>, для всего лишь 32к контекста нужно 400гб памяти на обычной llama.cpp.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/5de/c67/19b/5dec6719bbf3e6f84797bda523430bd8.png" width="904" height="387" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/5de/c67/19b/5dec6719bbf3e6f84797bda523430bd8.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/5de/c67/19b/5dec6719bbf3e6f84797bda523430bd8.png 781w" loading="lazy" decode="async"/></figure><p>Согласно тому же исследованию, качество кванта <a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF" rel="noopener noreferrer nofollow">UD-Q2_K_XL</a> (это динамическое квантование от Unsloth, главная особенность этого квантования в том, что важные тензоры оставлены в очень высоком качестве, а менее важные квантуются сильнее, за счет этого общее качество остается на высоком уровне) по сравнению с оригиналом FP8 падает всего на несколько процентов в бенчмарках:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/e79/021/938/e7902193855bc67679ba2b46faf91439.png" width="804" height="805" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/e79/021/938/e7902193855bc67679ba2b46faf91439.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/e79/021/938/e7902193855bc67679ba2b46faf91439.png 781w" loading="lazy" decode="async"/></figure><p>Только актуальная версия UD-Q2_K_XL занимает уже 233гб, а не 212гб, слишком много, но можно попробовать запустить её с ssd, это критично снизит скорость PP, но для средних контекстов всё еще может работать, например, 32к ждать пришлось минут 40:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/7b2/91b/0f3/7b291b0f377f40e1fdf0172eaf59e75d.png" alt="DeepSeek-R1-0528-UD-Q2_K_XL запуск частично с ssd, обработка 32к контекст, скорость 2.3 t/s" title="DeepSeek-R1-0528-UD-Q2_K_XL запуск частично с ssd, обработка 32к контекст, скорость 2.3 t/s" width="907" height="724" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/7b2/91b/0f3/7b291b0f377f40e1fdf0172eaf59e75d.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/7b2/91b/0f3/7b291b0f377f40e1fdf0172eaf59e75d.png 781w" loading="lazy" decode="async"/><div><figcaption>DeepSeek-R1-0528-UD-Q2_K_XL запуск частично с ssd, обработка 32к контекст, скорость 2.3 t/s</figcaption></div></figure><p>Кванты от ubergram по сути такое же динамическое квантование, только с использованием более продвинутых IQK и R4 квантов. </p><p>Но возвращаясь к тому, как получилось, что мы уже и тестировали и запускали 32к и для этого не понадобилось 400гб памяти, как это работает?</p><h2>MLA и Внимание</h2><p>Контекст так много весит, потому что он полностью связан, и постоянно динамически вычисляет важность токенов через механизм внимания. В отличии от таких статистических предсказателей следующего токена, вроде цепей Маркова, где предыдущее значение контекста не играет роли, играет только предыдущий токен, если это цепь 1 порядка, в трансформерах механизм внимания на каждом шагу работает со всей последовательностью, чтобы постоянно вычислять важность токенов и перепроверять, что ответ правильный. </p><p>Механизм Внимания - это квадратичная сложность <code>O(n²)</code>  и по памяти и по времени, где n-длина последовательности, каждый токен взаимодействует с каждым токеном, поэтому расход памяти так быстро растет. Чтобы справиться с этим, придумывают различные математические оптимизации, один из них это Flash Attention - используя математические трюки с матрицами уменьшается требования к памяти без потерь качества, результат идентичен обычному attention.</p><p>Или другой подход - Sliding Window Attention (SWA), когда токену ограничивают область видимости, например каждый токен видит вокруг себя только вокруг себя в пределах окна, которое обычно 4096 токенов, за счёт этого можно обрабатывать очень длинные последовательности используя мало памяти, но взамен теряется информация вне окна, что можно частично компенсировать различными техниками.</p><p>В DeepSeek пошли другим путем, они попытались изменить сам подход к attention. Обычно для оптимизации внимания используют, например, Grouped Query Attention (GQA) или Multi-Query Attention (MQA), эти методы являются вычислительными оптимизациями стандартного механизма без фундаментального изменения архитектуры. Вместо такого подхода в DeepSeek разработали MLA (Multi-Head Latent Attention), где роль для внимания играет не токен, а латентный вектор. </p><p>MLA - это обучение скрытых или латентных векторов вместе с основной моделью, эти вектора учатся улавливать ключевые концепции и паттерны в данных. Головы внимания в MLA взаимодействуют не напрямую с токенами, а с этими латентными векторами. За счет этого получается "ужать" KV-кэш в 25 раз сохраняя оригинальное качество:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/461/59b/548/46159b548ef07b417b78f0ad44b9c600.png" alt="MLA представили в DeepSeek V2: https://arxiv.org/abs/2405.04434" title="MLA представили в DeepSeek V2: https://arxiv.org/abs/2405.04434" width="863" height="381" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/461/59b/548/46159b548ef07b417b78f0ad44b9c600.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/461/59b/548/46159b548ef07b417b78f0ad44b9c600.png 781w" loading="lazy" decode="async"/><div><figcaption>MLA представили в DeepSeek V2: <a href="https://arxiv.org/abs/2405.04434" rel="noopener noreferrer nofollow">https://arxiv.org/abs/2405.04434</a></figcaption></div></figure><p>И причина, почему в том исследовании им потребовалось 400гб для 32к контекста в том, что на момент исследования в llama.cpp не была реализована поддержка MLA.</p><p>Кроме MLA, у DeepSeek есть ещё одна интересная технология - MTP.</p><p>MTP - это их реализация спекулятивного декодирования, способ переложить часть работы по предсказываю следующего токена на маленькую модель, это работает, когда продолжение слова или фразы уже очевидно. Если зайти на официальный репозиторий deepseek-ai, то размер модели будет 685B, а не 671B. Как раз 14B это <a href="https://arxiv.org/pdf/2412.19437" rel="noopener noreferrer nofollow">модуль MTP</a>. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/fde/a2d/b7a/fdea2db7a50d436bbf2ef77e5af5eb75.png" alt="Model size: 685B" title="Model size: 685B" width="1317" height="503" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/fde/a2d/b7a/fdea2db7a50d436bbf2ef77e5af5eb75.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/fde/a2d/b7a/fdea2db7a50d436bbf2ef77e5af5eb75.png 781w" loading="lazy" decode="async"/><div><figcaption>Model size: 685B</figcaption></div></figure><p>MTP, в отличии от обычного спекулятивного декодирования, тоже обучалось вместе с модель. По их замерам точность принятия токенов от MTP 85-90%, что дает ускорение основной модели в 1.8 раза.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/bed/402/eb8/bed402eb8dd4d567fd34377f9db28b88.png" width="750" height="253" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/bed/402/eb8/bed402eb8dd4d567fd34377f9db28b88.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/bed/402/eb8/bed402eb8dd4d567fd34377f9db28b88.png 781w" loading="lazy" decode="async"/></figure><p>В llama.cpp сейчас есть реализация <a href="https://github.com/ggerganov/llama.cpp/pull/10455" rel="noopener noreferrer nofollow">спекулятивного декодирования</a>, но она сделана по другому, там в качестве маленькой модели нужна полноценная модель того же семейства. Например, Gemma3 27B в качестве помощника может использовать только что-то из своих младших моделей, вроде Gemma3 1B.</p><h2>Сколько нужно памяти под контекст используя MLA</h2><p>В ik_llama, чтобы включить использование mla нужно добавить параметр <code>-fa -mla 3</code> . </p><p>В llama.cpp все виды внимания (mla, swa) используемые конкретной моделью включаются через <code>-fa</code> автоматически, если их поддержка уже добавлена в ядро.</p><p>Тензоры оставленные с -ot для кванта IQ1_S_R4 примерно равны 10.5гб, под нужны ОС, включая браузер, на 4090 уходит около 1.7гб. В итоге есть примерно 5гб и 11гб под контекст на 4060ti и 4090 соответственно. Перебором параметра <code>-c</code> можно найти количество контекста, которое влезает в этот объем. Чем больше размер батча, тем больше под них нужно памяти.</p><p>Размер батчей стандартный -b 2048 -ub 512:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/b55/bc9/c75/b55bc9c750990ad4ea0064428d09f53a.png" alt="Тензоры выгружены на GPU, b/ub стандартные -b 2048 -ub 512" title="Тензоры выгружены на GPU, b/ub стандартные -b 2048 -ub 512" width="934" height="151" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/b55/bc9/c75/b55bc9c750990ad4ea0064428d09f53a.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/b55/bc9/c75/b55bc9c750990ad4ea0064428d09f53a.png 781w" loading="lazy" decode="async"/><div><figcaption>Тензоры выгружены на GPU, b/ub стандартные -b 2048 -ub 512</figcaption></div></figure><p>Размер батчей -b 4096 -ub 4096:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/091/0bc/03c/0910bc03ccb65ec1f3dbcae07bf5c761.png" alt="Тензоры выгружены на GPU, под контекст остатки памяти, -b 4096 -ub 4096" title="Тензоры выгружены на GPU, под контекст остатки памяти, -b 4096 -ub 4096" width="940" height="152" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/091/0bc/03c/0910bc03ccb65ec1f3dbcae07bf5c761.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/091/0bc/03c/0910bc03ccb65ec1f3dbcae07bf5c761.png 781w" loading="lazy" decode="async"/><div><figcaption>Тензоры выгружены на GPU, под контекст остатки памяти, -b 4096 -ub 4096</figcaption></div></figure><p>Как вариант, можно не выгружать тензоры на GPU совсем, оставляя всю память под контекст. Для этого надо указать<code>-ngl 0</code>. Максимальные 160к без квантования требуют примерно 14гб при стандартном размере батчей и 18гб при 4096.</p><p>Запустим тот же бенчмарк, контекст 160к, все тензоры теперь на CPU:</p><p><code>CUDA_VISIBLE_DEVICES="0" ./llama-sweep-bench -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 0 -ctk q8_0 -t 28 -c 163840 -ngl 31 -b 4096 -ub 4096</code></p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/426/e5b/144/426e5b1443fa9a83a5fcdb7883504b66.png" alt="" title="" width="943" height="150" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/426/e5b/144/426e5b1443fa9a83a5fcdb7883504b66.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/426/e5b/144/426e5b1443fa9a83a5fcdb7883504b66.png 781w" loading="lazy" decode="async"/></figure><p>Скорость tg упала до скорости 2.85 t/s и соответствует "CPU only". Но с ростом контекста она быстро упадет до 1, а под конец и до 0.5 t/s. А скорость PP осталась как и была на GPU, около 200-300 t/s, но под конец 160к она упадёт до 75 t/s.</p><p>Теперь проверим "сложение" двух GPU. Ожидание от двух карт такое, что можно разместить количество контекста сумме их отдельных размеров, то есть в 126к. Но на практике объединив две GPU можно уместить всего 110к с квантованием q8_0, что не совпадает с расчетами. Дело в том, что единственная польза от двух карт в том, что мы разделяем тензоры, отправляя 6гб на gpu1 и 3гб на gpu2, тем самым освобождая память для контекста, а сам контекст создается на двух GPU почти пропорционального размера, то есть не получится сложить 40к + 86к. Логика работы двух GPU и контекста мне пока не понятна.</p><p>Замер скорости двух GPU и контекста 110к:</p><p><code>./llama-sweep-bench -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 99 -ts 20,10 -b 4096 -ub 4096 -ctk q8_0 -t 28 -c 112640</code> </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/803/d8d/dfb/803d8ddfb7bcf5bc71659b95da363fa9.png" alt="4090 + 4060ti, 110k контекста" title="4090 + 4060ti, 110k контекста" width="666" height="242" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/803/d8d/dfb/803d8ddfb7bcf5bc71659b95da363fa9.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/803/d8d/dfb/803d8ddfb7bcf5bc71659b95da363fa9.png 781w" loading="lazy" decode="async"/><div><figcaption>4090 + 4060ti, 110k контекста</figcaption></div></figure><p>Скорости pp просели из-за медленной 4060, но всё еще пригодны для использования и не заставляют ждать слишком долго.</p><h2>Загружаем модель и проводим тесты</h2><p>Теперь когда теории достаточно, как ускориться и как вместить огромный контекст понятно, осталось придумать как его проверить. Кодовой базы на 100к у меня нет, а работать с чужой - сложно оценить результат.</p><p>Ещё один из вариантов проверить такой большой контекст - это взять знакомую книгу и протестировать модель на ней, сможет ли модель пересказать всю книгу, выдать какие-то факты из начала, середины и конца, и в целом по ответам будет понятно, модель способна обработать такой контекст или она просто выбирает случайные подходящие слова и выдает какой-то не особо связный ответ. Часто маленькие модели (7b, 12b, 14b) уже на контексте в 32к зацикливаются и просто выдают бесконечно одинаковый токен.</p><p>Нужна книга, которой точно нет в обучающем датасете, например, Лабиринт Отражений. Текст книги нужно целиком вставить в системный промпт, тогда это будет контекст модели, а потом позадавать вопросы по нему. Это будет не RAG, который разбивает текст на кусочки, создаёт векторную БД и разбивает данные на кусочки, и по ключевым словам достает эти кусочки и подмешивает их в контекст, такие кусочки не связаны друг с другом единым вниманием, поэтому такой подход подходит для документации, но не для связной книги. И это не какой-то похожий подход, это будет чистый контекст. </p><p>Проверка модели с пустым системным промптом в своем "базовом" виде. Она ничего не знает о Лабиринте Отражений, но что-то знает про цикл Дозоры, что совсем не помогает.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/e88/845/ad9/e88845ad9fb379e34bcca7c41548d68f.png" alt="Пустой системный промпт" title="Пустой системный промпт" width="1071" height="600" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/e88/845/ad9/e88845ad9fb379e34bcca7c41548d68f.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/e88/845/ad9/e88845ad9fb379e34bcca7c41548d68f.png 781w" loading="lazy" decode="async"/><div><figcaption>Пустой системный промпт</figcaption></div></figure><p>Стоит подсчитать количество токенов из которого состоит книга, для этого можно воспользоваться утилитой llama-tokenize, и нужно указать модель, так как у всех моделей разные токенизаторы.</p><p>linux:</p><p><code>./llama-tokenize -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -f book.txt | wc -l</code></p><p>windows powershell:</p><p><code>.\llama-tokenize.exe -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -f book.txt | Measure-Object -Line | Select-Object -ExpandProperty Lines</code></p><p>Размер книги получился 215к токенов, не влезает ни в 110к, ни даже в максимальные 160к. В таком случае, когда фактический контекст больше размера <code>-c</code>  подключается context shift, он обрезает часть токенов и это сильно снижает точность и качество. Проверим как это работает, а потом урежем контекст книги.</p><p>В системный промпт скопирован текст всей книги, context shift включается автоматически, попросим пересказать сюжет:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/1dc/a56/00f/1dca5600f818608b78d8224c7deb1ce3.png" width="1073" height="930" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/1dc/a56/00f/1dca5600f818608b78d8224c7deb1ce3.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/1dc/a56/00f/1dca5600f818608b78d8224c7deb1ce3.png 781w" loading="lazy" decode="async"/></figure><p>Первое, что удивляет, это то, что настолько экстремально квантованная модель что-то отвечает на таком огромном контексте. </p><p>Второе - то, она отвечает что-то связное, описание в целом пересказано верно, ключевая особенность героя передана верно, ключевой момент с Неудачником описан верно. Но видно, что данные из первой половины книги не учтены, это работа context shift.</p><p>Посмотреть на то, как context shift даёт негативный эффект, можно спросив какую-то конкретную вещь из книги. Например, попросить точно процитировать эпиграф из самого начала. В ответе полностью выдуманный текст, ничего общего с оригиналом:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/cce/df4/cb7/ccedf4cb7e786eb4a1506dc5656601be.png" alt="context shift не позволяет точно цитировать" title="context shift не позволяет точно цитировать" width="1018" height="502" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/cce/df4/cb7/ccedf4cb7e786eb4a1506dc5656601be.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/cce/df4/cb7/ccedf4cb7e786eb4a1506dc5656601be.png 781w" loading="lazy" decode="async"/><div><figcaption>context shift не позволяет точно цитировать</figcaption></div></figure><p>Урежем книгу до 100к токенов, теперь текст полностью влезает в контекст. Зададим тот же вопрос, и да, теперь ответ правильный:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/939/ece/919/939ece919421b269acfc548d0dfb01f9.png" alt="110к, без context shift, цитата верна" title="110к, без context shift, цитата верна" width="1064" height="619" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/939/ece/919/939ece919421b269acfc548d0dfb01f9.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/939/ece/919/939ece919421b269acfc548d0dfb01f9.png 781w" loading="lazy" decode="async"/><div><figcaption>110к, без context shift, цитата верна</figcaption></div></figure><p>Проверим способность MLA и модели работать не только с началом и концом, возьмём цитату из середины и попросим модель найти откуда эта цитата, описать что происходит в этот момент и процитировать диалог целиком.</p><blockquote><p>В каком месте книги кто-то сказал "Звёзды слишком яркие."? Процитируй весь диалог</p></blockquote><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/ddf/cb4/d49/ddfcb4d49e5ed7739e5f21853eaddcb2.png" alt="100к контекста, цитирование из середины книги, диалог дословно верный" title="100к контекста, цитирование из середины книги, диалог дословно верный" width="931" height="511" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/ddf/cb4/d49/ddfcb4d49e5ed7739e5f21853eaddcb2.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/ddf/cb4/d49/ddfcb4d49e5ed7739e5f21853eaddcb2.png 781w" loading="lazy" decode="async"/><div><figcaption>100к контекста, цитирование из середины книги, диалог дословно верный</figcaption></div></figure><p>Описание событий правильное, цитирование диалога правильное. Ошибка только в том, что это глава 010, а не 011. Возможно модель сбивает то, что нумерация глав в книге представлена в двоичном виде. </p><h2>160к контекста на одной 4090</h2><p>Осталось только проверить максимальный контекст для DeepSeek R1, который составляет 160к. Контекст PP будет считаться на GPU, а новые токены ответа TG на CPU, и сколько останется памяти, догрузим GPU целыми слоями.</p><p>Если в предыдущем эксперименте скорости были довольно комфортные, то тут скорость будет очень низкой. Это больше про посмотреть, остаётся ли ответ модели разумным на максимальном контексте в таком экстремальном квантовании.</p><p><code>./llama-server -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 18 -ts 24,0 -ctk q8_0 -ctv q8_0 -b 4096 -ub 4096 -t 28 -c 163840</code></p><p>Удалось выгрузить 18 слоев, для большей экономии включил <code>-ctv q8_0</code>, по идее это не должно сказаться на качестве, а несколько гб высвободиться. </p><p>Загрузка контекста успешно проходит до 128к и тут мы сталкиваемся с первой проблемой:</p><p>```cpy.cu:573: GGML_ASSERT(ggml_nbytes(src0) &lt;= INT_MAX) failed```</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/1e3/18a/f12/1e318af1282163f372005e30f02d1129.png" alt="Логи прогрузки 160к контекста" title="Логи прогрузки 160к контекста" width="1235" height="668" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/1e3/18a/f12/1e318af1282163f372005e30f02d1129.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/1e3/18a/f12/1e318af1282163f372005e30f02d1129.png 781w" loading="lazy" decode="async"/><div><figcaption>Логи прогрузки 160к контекста</figcaption></div></figure><p>Из-за размера батча 4096 мы достигли и превысили лимит INT_MAX. Возможно это исправить или нет, пока не ясно, поэтому придется снизить -b -ub, что замедлит скорость PP, но на данный момент это единственный способ загрузить столько контекста. Уменьшая размер батча, высвободиться видеопамять, значит можно выгрузить больше слоев на GPU, что может немного поможет при генерации ответа:</p><p><code>./llama-server -m "DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 30 -ts 24,0 -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 -t 28 -c 163840</code></p><p>Скорость PP под конец упала до 75 t/s, а скорость генерации составила всего 0.5 t/s:</p><figure class=""><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/f04/3f9/613/f043f9613dcd813280abd4659e775567.png" width="512" height="150" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/f04/3f9/613/f043f9613dcd813280abd4659e775567.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/f04/3f9/613/f043f9613dcd813280abd4659e775567.png 781w" loading="lazy" decode="async"/></figure><p>Зато можно убедиться, что на 160к модель всё еще не утратила связь с контекстом, и сам ответ правильный в пределах этих 160к, так как развязка осталась в оставшихся 55к.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/f11/a69/866/f11a69866359b2e515288c5ea9fa82ae.png" width="956" height="545" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/f11/a69/866/f11a69866359b2e515288c5ea9fa82ae.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/f11/a69/866/f11a69866359b2e515288c5ea9fa82ae.png 781w" loading="lazy" decode="async"/></figure><p>Чтобы воспользоваться всем контекстом и выгрузить все слои на GPU не хватило буквально 4-5гб, поэтому эту работу оставим для 5090, а для 4090 пределом будет 80к.</p><h2>Бонус. Огромный контекст на LLama 4 и Gemma3</h2><p>DeepSeek не единственная модель, в которую можно загрузить много контекста, но единственная у которой есть MLA. Для других моделей стандартным решением является SWA - скользящее окно внимания.</p><p>Недавно в llama.cpp добавили <a href="https://github.com/ggml-org/llama.cpp/pull/13194" rel="noopener noreferrer nofollow">поддержку SWA</a>, которое позволяет обрабатывать огромные последовательности требуя мало памяти. Качество должно быть ниже чем у MLA, так как уходя за пределы окна происходит очистка SWA-кэша и, после определенной фиксации в пределах окна, забывание токенов данных, но проверить всё равно можно. Этой поддержки пока нет в ik_llama, поэтому запускать надо на llama.cpp.</p><p>SWA работает и для Llama 4 и для Gemma3, включается через <code>-fa</code>, есть возможность включить <code>--swa-full</code>, в этом режиме размер кэша SWA равен полному контексту, очищения SWA кэша не происходит, но памяти потребует намного больше. При использовании SWA context shift автоматически отключается.</p><p>У Llama 4 Scout (108B-A17B) размер контекста 10м, У Llama 4 Maverick (401B-A17B) - 1м. Этого хватит, чтобы вместить книгу целиком. У Gemma3 27B только 128к, но она славится тем, чем её контекст очень тяжелый, поэтому SWA должен с этим помочь.</p><p>Gemma3 это Dense модель, а Llama 4 это MoE, у Scout 16 экспертов, а у Maverick 128. У Llama 4 есть много общих слоев, поэтому эти модели выдают очень хорошую скорость при использовании <code>-ot</code>, а Gemma3 в целом легковесная.</p><p>Для начала посмотрим сколько токенизаторы Llama 4 и Gemma3 найдут токенов у книги:</p><p><code>./llama-tokenize -m "Llama-4-Maverick-17B-128E-Instruct-UD-Q3_<br/>K_XL-00001-of-00004.gguf" -f "<u>labir_otra.txt"</u> | wc -l</code><br/><code>&gt; 182120</code></p><p><code>.\llama-tokenize.exe -m "gemma-3-27b-it-Q4_K_M.gguf" -f "labir_otra.txt" | Measure-Object -Line | Select-Object -ExpandProperty Lines</code><br/><code>&gt; 190549</code></p><p>177к токенов у Llama4 и 186к у Gemma3. Видимо у них токенизатор лучше подходит для текста на русском, но как это скажется на качестве пока не ясно. </p><p>Модели запускаем через llama.cpp, логика такая же как и раньше, только нужно убрать параметры которых нет в llama.cpp. Модели не запускаются с <code>-fa</code>, если квантование кэшей не синхронно, поэтому придётся указать и <code>-ctv q8_0</code>:</p><h4>Llama 4 Maverick</h4><p>Максимально можно вместить 350к контекста в стандартных размераз ub/b и примерно 210к для <code>-ub 3072 -b 3072</code></p><p>Квант UD-Q3_K_XL.</p><p><code>./llama-server -m "Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00001-of-00004.gguf" -fa -ctk q8_0 -ctv q8_0 -c 215040 -ot exps=CPU -ngl 99 -ts 24,0 -t 28</code></p><p>Модель правильно собрала вместе части, но в 3 части 8 глав, а не 4. Видимо модель сбивает нумерация в бинарном виде, даже если явно ей это сказать. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/ee5/244/f88/ee5244f88aa2731992a971c84f56f810.png" alt="" title="" width="952" height="718" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/ee5/244/f88/ee5244f88aa2731992a971c84f56f810.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/ee5/244/f88/ee5244f88aa2731992a971c84f56f810.png 781w" loading="lazy" decode="async"/></figure><p>Проверим точность цитирования из середины. Цитата точная, описание не совсем точное. Глава указана правильно, а название части модель выдумала по тому, где находятся герои, правильное название "ЧАСТЬ ВТОРАЯ. ЛАБИРИНТ":</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/9d3/f40/61c/9d3f4061ca797cb9a7c5eb5b79542d36.png" alt="Llama 4 Maverick, книга целиком в контексте" title="Llama 4 Maverick, книга целиком в контексте" width="928" height="666" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/9d3/f40/61c/9d3f4061ca797cb9a7c5eb5b79542d36.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/9d3/f40/61c/9d3f4061ca797cb9a7c5eb5b79542d36.png 781w" loading="lazy" decode="async"/><div><figcaption>Llama 4 Maverick, книга целиком в контексте</figcaption></div></figure><p>Попробуем запутать модель, она должна понять, что пивной ларёк это на самом деле бар и упомянуть про "Ждите отстоя пены".</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/b14/5ac/07d/b145ac07d81439bbef2818d9ebd04170.png" alt="llama 4 maverick, ответы правильные" title="llama 4 maverick, ответы правильные" width="985" height="909" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/b14/5ac/07d/b145ac07d81439bbef2818d9ebd04170.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/b14/5ac/07d/b145ac07d81439bbef2818d9ebd04170.png 781w" loading="lazy" decode="async"/><div><figcaption>llama 4 maverick, ответы правильные</figcaption></div></figure><p> С этим модель справляется успешно. То есть в пределах SWA окна у модели нет проблем.</p><p>Теперь вопрос на контексте 32к, чтобы на нём же протестировать более маленькие модели.</p><blockquote><p>В какой момент была включена Roll Over Beethoven? Что произошло дальше?</p></blockquote><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/78d/8b0/286/78d8b0286ebb21cc5a7b6baf2926d74c.png" alt="Llama 4 Maverick, правильный ответ для тестирования" title="Llama 4 Maverick, правильный ответ для тестирования" width="929" height="369" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/78d/8b0/286/78d8b0286ebb21cc5a7b6baf2926d74c.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/78d/8b0/286/78d8b0286ebb21cc5a7b6baf2926d74c.png 781w" loading="lazy" decode="async"/><div><figcaption>Llama 4 Maverick, правильный ответ для тестирования</figcaption></div></figure><p>Тут модель дает правильный ответ, всё это в пределах одной главы.</p><p>Небольшой тест 4060 ti, в неё влезает 70к контекста с ub/b 4096. Обработка pp очень медленная, но генерация ответа вполне быстрая. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/2e9/ad1/877/2e9ad187775e5730da04c60190d3f448.png" alt="4060 ti с контекстом 70к, скорость pp низкая, скорость tg нормальная" title="4060 ti с контекстом 70к, скорость pp низкая, скорость tg нормальная" width="1082" height="211" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/2e9/ad1/877/2e9ad187775e5730da04c60190d3f448.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/2e9/ad1/877/2e9ad187775e5730da04c60190d3f448.png 781w" loading="lazy" decode="async"/><div><figcaption>4060 ti с контекстом 70к, скорость pp низкая, скорость tg нормальная</figcaption></div></figure><p>Ответ правильный, так как ответ снова в пределах абзатца.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/4fe/5a4/910/4fe5a491096df8cd4699d67b05a8b473.png" alt="4060 ti с контекстом 70к, модель правильно нашла место" title="4060 ti с контекстом 70к, модель правильно нашла место" width="911" height="261" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/4fe/5a4/910/4fe5a491096df8cd4699d67b05a8b473.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/4fe/5a4/910/4fe5a491096df8cd4699d67b05a8b473.png 781w" loading="lazy" decode="async"/><div><figcaption>4060 ti с контекстом 70к, модель правильно нашла место</figcaption></div></figure><h4>Llama 4 Scout</h4><p>Maverick в целом справляется, если знать особенности SWA. Но вот у Scout с этим намного хуже, ответы хаотичные и случайные, иногда попадающие куда надо. Видимо сказывается тот факт, что у Maverick 128 экспертов, а у Scout только 16. </p><p>Квант UD-Q4_K_XL. </p><p>Тот же вопрос в пределах 32к. Ответ не правильный по своей логике, Scout пишет, что герой уже в ресторане и там и взял диск, что не правильно, и после заказывает такси, чтобы поехать в тот самый ресторан, где он уже находится по версии модели. Видно, что модель слабее Maverick и не может удерживать события даже в своем ответе.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/2c2/6f4/b2f/2c26f4b2f83a2b97acae6e7738139707.png" width="884" height="363" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/2c2/6f4/b2f/2c26f4b2f83a2b97acae6e7738139707.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/2c2/6f4/b2f/2c26f4b2f83a2b97acae6e7738139707.png 781w" loading="lazy" decode="async"/></figure><h4>Gemma3</h4><p>У Gemma3 возникает проблема на длинном контексте без квантования. Если контекст длиннее 32к, то вместо ответа получается зацикленный токен, эта проблема возникает только без квантования kv-кэша. В пределах 32к контекста ответы более менее нормальные, 32к контекст занимает 2.5гб без квантования.</p><p>Если включить квантование <code>-ctk q8_0 -ctv q8_0</code>, то полный 128к контекст обрабатывается и занимает ~7гб, но модель слишком маленькая, всего 27B, поэтому на таком огромном контексте она показывает себя плохо.</p><p>Квант Q8_0, модель не удерживает даже 32к контекста и выдумывает ответ из обрывков фраз книги, про то, что герой вернулся в реальность. Возможно, это проблема реализации SWA, но включение <code>--swa-full</code> не помогает, что говорит о том, что это проблема модели.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/4e2/ac1/0a3/4e2ac10a3313f4cf0a4a682b2cc3af90.png" alt="Gemma3 27b, 32к, ответ не правильный" title="Gemma3 27b, 32к, ответ не правильный" width="943" height="478" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/4e2/ac1/0a3/4e2ac10a3313f4cf0a4a682b2cc3af90.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/4e2/ac1/0a3/4e2ac10a3313f4cf0a4a682b2cc3af90.png 781w" loading="lazy" decode="async"/><div><figcaption>Gemma3 27b, 32к, ответ не правильный</figcaption></div></figure><h2>Если не хочется возиться с консолью</h2><h4>oobabooga / text-generation-webui</h4><p>Пока <a href="https://github.com/oobabooga/text-generation-webui" rel="noopener noreferrer nofollow">text-generation-webui</a> единственный клиент, где реализовали поддержку кастомных параметров командной строки, включая <code>-ot</code>. Тут нет поддержки ik_llama, но для UD-квантов DeepSeek или Llama 4 подойдет. Правда размер контекста тут ограничен 128к.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/a96/c2a/1c3/a96c2a1c3bdeb8cb87bb0754ba952f60.png" width="1513" height="961" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/a96/c2a/1c3/a96c2a1c3bdeb8cb87bb0754ba952f60.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/a96/c2a/1c3/a96c2a1c3bdeb8cb87bb0754ba952f60.png 781w" loading="lazy" decode="async"/></figure><h2>OpenAI API, Jan и Cherry Studio</h2><p>При запуске llama-server у ik_llama или llama.cpp создается не только веб-клиент, но и openai compatiable api, поэтому можно пользоваться любым клиентом, который умеет подключаться к openai. Два клиента с открытым кодом, который так умеют: <a href="https://jan.ai/" rel="noopener noreferrer nofollow">Jan</a> и <a href="https://github.com/CherryHQ/cherry-studio" rel="noopener noreferrer nofollow">Cherry Studio</a>.</p><p>К url веб-клиента нужно просто добавить /v1 и получить адрес api, который можно использовать в любом софте, в том числе и таком как Cline или Continue для разработки.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/941/4dc/f66/9414dcf66af59bff7b7f5b8071921e9b.png" width="1017" height="539" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/941/4dc/f66/9414dcf66af59bff7b7f5b8071921e9b.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/941/4dc/f66/9414dcf66af59bff7b7f5b8071921e9b.png 781w" loading="lazy" decode="async"/></figure><h2>Вывод</h2><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/589/306/148/589306148d255c0eead9849fbc0230ff.png" alt="" title="" width="929" height="473" sizes="(max-width: 780px) 100vw, 50vw" srcset="https://habrastorage.org/r/w780/getpro/habr/upload_files/589/306/148/589306148d255c0eead9849fbc0230ff.png 780w,&#10;       https://habrastorage.org/r/w1560/getpro/habr/upload_files/589/306/148/589306148d255c0eead9849fbc0230ff.png 781w" loading="lazy" decode="async"/></figure><ul><li><p>Для ускорения MoE моделей (LLama 4, DeepSeek, Qwen3) нужен параметр -ot и одна GPU, это позволит получить ощутимое ускорение</p></li><li><p>Чтобы вместить огромный контекст в небольшой объём памяти нужно использовать MLA для DeepSeek и SWA для Llama 4 / Gemma3</p></li><li><p>4060 ti позволит вмещать 32к контекста в DeepSeek и 70к в Llama 4 Maverick, но обрабатывает долговато, медленная память сказывается</p></li><li><p>4090 способна вместить 80к и обрабатывать контекст на комфортной скорости 200-300 t/s, в Maverick способна вместить 210к</p></li><li><p>Даже на максимальном для R1 контексте 160к мини-квант отвечает нормально</p></li></ul><p>Эксперимент с огромным контекстом прошел лучше чем задумывался, ожидания были, что где-то уже после 8к, максимум 32к, модель совсем перестанет связно отвечать и будет много артефактов. Но не стоит ожидать, что такой маленький квант будет соответствовать качеству оригинала, хотя он и справляется лучше, чем многие другие модели.</p></div></div></div><!----><!----></div><!----><!----></div><!--]--><!----><div class="tm-article-presenter__meta" data-test-id="article-meta-links"><div class="tm-separated-list tag-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span><ul class="tm-separated-list__list"><!--[--><li class="tm-separated-list__item"><!--[--><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=[llama.cpp]" class="tm-tags-list__link"><span>llama.cpp</span></a><!--]--></li><li class="tm-separated-list__item"><!--[--><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=[ik_llama]" class="tm-tags-list__link"><span>ik_llama</span></a><!--]--></li><li class="tm-separated-list__item"><!--[--><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=[deepseek]" class="tm-tags-list__link"><span>deepseek</span></a><!--]--></li><li class="tm-separated-list__item"><!--[--><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=[%D0%BB%D0%BE%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5+%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B8]" class="tm-tags-list__link"><span>локальные нейросети</span></a><!--]--></li><li class="tm-separated-list__item"><!--[--><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=[deepseek+r1]" class="tm-tags-list__link"><span>deepseek r1</span></a><!--]--></li><li class="tm-separated-list__item"><!--[--><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=[deepseek+v3]" class="tm-tags-list__link"><span>deepseek v3</span></a><!--]--></li><!--]--><!----></ul></div><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span><ul class="tm-separated-list__list"><!--[--><li class="tm-separated-list__item"><!--[--><a href="/ru/hubs/artificial_intelligence/" class="tm-hubs-list__link"><!--[--><span>Искусственный интеллект</span><!--]--></a><!--]--></li><!--]--><!----></ul></div></div><!----><!--]--></article><!--]--></div><!----></div><div style="" class="tm-article-sticky-panel" data-test-id="article-sticky-panel"><div class="tm-data-icons tm-data-icons tm-data-icons_space-big tm-article-sticky-panel__icons" data-test-id="article-stats-icons"><div class="article-rating tm-data-icons__item" data-v-08e911e5><div class="tm-votes-lever tm-votes-lever tm-votes-lever_appearance-article votes-switcher" title="Всего голосов 89: ↑89 и ↓0" data-v-08e911e5><button class="tm-votes-lever__button" data-test-id="votes-lever-upvote-button" title="Нравится" type="button"><svg class="tm-svg-img tm-votes-lever__icon" height="24" width="24"><title>Нравится</title><use xlink:href="/img/megazord-v28.7909a852..svg#counter-vote"></use></svg></button><div class="tm-votes-lever__score tm-votes-lever__score_appearance-article tm-votes-lever__score_no-margin tm-votes-lever__score"><!--[--><span><span class="tm-votes-lever__score-counter tm-votes-lever__score-counter_positive tm-votes-lever__score-counter" data-test-id="votes-score-counter">+109</span></span><!--]--></div><button class="tm-votes-lever__button" data-test-id="votes-lever-downvote-button" title="Не нравится" type="button"><svg class="tm-svg-img tm-votes-lever__icon tm-votes-lever__icon_arrow-down" height="24" width="24"><title>Не нравится</title><use xlink:href="/img/megazord-v28.7909a852..svg#counter-vote"></use></svg></button></div><!--teleport start--><!--teleport end--><!----></div><!----><!----><button class="bookmarks-button tm-data-icons__item" title="Добавить в закладки" type="button"><span class="tm-svg-icon__wrapper bookmarks-button__icon"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Добавить в закладки</title><use xlink:href="/img/megazord-v28.7909a852..svg#counter-favorite"></use></svg></span><span class="bookmarks-button__counter" title="Количество пользователей, добавивших публикацию в закладки">149</span></button><div class="tm-sharing tm-data-icons__item" title="Поделиться"><button class="tm-sharing__button" type="button"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="tm-sharing__icon"><path fill="currentColor" d="M13.8 13.8V18l7.2-6.6L13.8 5v3.9C5 8.9 3 18.6 3 18.6c2.5-4.4 6-4.8 10.8-4.8z"></path></svg></button><!--teleport start--><!--teleport end--></div><div class="tm-article-comments-counter-link tm-data-icons__item" title="Читать комментарии"><a href="/ru/articles/921540/comments/" class="tm-article-comments-counter-link__link" data-test-id="counter-comments"><!--[--><svg class="tm-svg-img tm-article-comments-counter-link__icon" height="24" width="24"><title>Комментарии</title><use xlink:href="/img/megazord-v28.7909a852..svg#counter-comments"></use></svg><span class="tm-article-comments-counter-link__value">39</span><!--]--></a><!----></div><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><!--teleport start--><!--teleport end--><!----></div></div></div><!--[--><!--]--><div class="tm-article-presenter__footer"><!--[--><!--[--><div class="tm-article-blocks"><!----><!--[--><section class="tm-block tm-block tm-block_spacing-bottom"><!----><!--[--><div class="tm-block__body tm-block__body tm-block__body_variant-balanced"><!--[--><div class="tm-article-author" data-test-id="article-author-info" data-async-called="true"><!--[--><!--]--><div class="tm-user-card tm-user-card tm-user-card_variant-article tm-article-author__user-card" data-async-called="true"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/Shannon/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><!--[--><img alt="" class="tm-entity-image__pic" src="https://assets.habr.com/habr-web/img/avatars/126.png"><!--]--></div></a><div class="tm-user-card__meta"><div class="tm-counter-container karma" title=" 176 голосов " data-v-ffa1e21e><div class="tm-counter-container__header"><!--[--><div class="karma-display positive" data-v-ffa1e21e data-v-7635202e>146</div><!----><!--]--></div><div class="tm-counter-container__footer"><!--[--><div class="karma-text" data-v-ffa1e21e>Карма</div><!--teleport start--><!--teleport end--><!--]--></div></div><div class="tm-counter-container" title="Рейтинг пользователя"><div class="tm-counter-container__header"><!--[--><!--[--><!--]--><div class="tm-votes-lever tm-votes-lever tm-votes-lever_appearance-rating"><!----><div class="tm-votes-lever__score tm-votes-lever__score_appearance-rating tm-votes-lever__score"><!--[--><span><span class="tm-votes-lever__score-counter tm-votes-lever__score-counter_rating tm-votes-lever__score-counter" data-test-id="votes-score-counter">56.6</span></span><!--]--></div><!----></div><!--]--></div><div class="tm-counter-container__footer"><!--[--><span class="tm-rating__text tm-rating__text">Рейтинг</span><!--]--></div></div></div></div></div><div class="tm-user-card__info tm-user-card__info_variant-article tm-user-card__info"><div class="tm-user-card__title tm-user-card__title_variant-article tm-user-card__title"><!----><a href="/ru/users/Shannon/" class="tm-user-card__nickname tm-user-card__nickname tm-user-card__nickname_variant-article"> @Shannon</a><!----></div><p class="tm-user-card__short-info tm-user-card__short-info_variant-article tm-user-card__short-info" data-test-id="user-card-speciality">Пользователь</p></div></div><!----><div class="tm-user-card__buttons tm-user-card__buttons_variant-article tm-user-card__buttons"><!----><div class="tm-user-card__button"><div class="tm-button-follow tm-user-card__button-follow"><!----><button class="tm-button-follow__button tm-button-follow__button_big" data-test-id="follow-button" type="button">Подписаться</button></div></div><!----><div class="tm-user-card__button tm-user-card__button_write" data-test-id="user-card-conversations"><svg class="tm-svg-img tm-user-card__button-icon" height="16" width="16"><title>Отправить сообщение</title><use xlink:href="/img/megazord-v28.7909a852..svg#mail"></use></svg></div><!----></div><!----></div><div class="tm-article-author__user-contacts" data-test-id="author-contacts"><!----><!----><!----></div></div><!--]--></div><!--]--><!----></section><!----><!--[--><div class="banner-wrapper leaderboard tm-page-article__banner" style="--d33b7e1a:200px;--411af8d8:auto;" data-v-0fffe3b8><!--[--><div class="placeholder-wrapper placeholder" data-v-0fffe3b8><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div class="adfox-banner-placeholder leaderboard" data-v-12f7bcca><div class="image loads" data-v-12f7bcca></div><div class="lines" data-v-12f7bcca><div class="line loads" data-v-12f7bcca></div><div class="line loads" data-v-12f7bcca></div><div class="line loads" data-v-12f7bcca></div></div></div><!----><!----><!----></div><div id="adfox_164725660339535756" class="tm-adfox-banner" data-v-0fffe3b8></div><!--]--></div><!--]--><!--]--><div class="tm-article-blocks__comments"><div id="publication-comments" class="tm-article-page-comments"><div><!--[--><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/articles/921540/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style" data-test-id="counter-comments"><!--[--><svg class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted" height="24" width="24"><title>Комментарии</title><use xlink:href="/img/megazord-v28.7909a852..svg#counter-comments"></use></svg><span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted"> Комментарии 39 </span><!--]--></a><!----></div><!--]--></div></div></div><!--[--><!--[--><!--]--><section class="tm-block tm-block tm-block_spacing-bottom"><header class="tm-block__header tm-block__header tm-block__header_variant-borderless"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title tm-block__title_variant-large">Публикации</h2><!--[--><!--]--></div><!----></header><!--[--><div class="tm-block__body tm-block__body tm-block__body_variant-condensed-slim"><!--[--><!--[--><div class="tm-tabs tm-tabs"><div class=""><!--[--><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link_active tm-tabs__tab-link_slim tm-tabs__tab-link">Лучшие за сутки</button></span><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link_slim tm-tabs__tab-link">Похожие</button></span><!--]--></div><!----></div><div class="similar-and-daily__tab-view"><div class="daily-articles-list"><ul class="tm-article-card-list"><!--[--><!--]--><div class="tm-bordered-card"><!----><!--[--><!--]--></div></ul><div class="daily-articles-block__button-container"><button class="btn btn_transparent btn_small tm-button tm-button_color-horizon" type="button"><!--[--><!--[-->Показать лучшие за всё время<!--]--><!--]--></button></div></div><!----></div><!--]--><!--]--></div><!--]--><!----></section><!--[--><div><div class="placeholder-wrapper"><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div class="tm-placeholder-promo"><div class="tm-placeholder-promo__header"><div class="tm-placeholder__line tm-placeholder__line_promo-title"></div></div><div class="tm-placeholder-promo__body"><div class="tm-placeholder-promo__posts"><div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div><div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div><div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div><div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div><div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div><div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div></div><div class="tm-placeholder-promo__dots"><div class="tm-placeholder-promo__dot"></div><div class="tm-placeholder-promo__dot"></div><div class="tm-placeholder-promo__dot"></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div></div><div class="placeholder-wrapper"><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div class="tm-placeholder-inset tm-placeholder-questions"><div class="tm-placeholder-inset__header"><div class="tm-placeholder__line tm-placeholder__line_inset-header loads"></div></div><div class="tm-placeholder-inset__body"><ul class="tm-placeholder-list"><!--[--><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div><div class="tm-project-block-items__properties"><!--[--><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><!--]--></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div><div class="tm-project-block-items__properties"><!--[--><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><!--]--></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div><div class="tm-project-block-items__properties"><!--[--><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><!--]--></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div><div class="tm-project-block-items__properties"><!--[--><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><!--]--></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div><div class="tm-project-block-items__properties"><!--[--><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width:100px;"></span></span><!--]--></div></li><!--]--></ul></div><div class="tm-placeholder-inset__footer"><div class="tm-placeholder__line tm-placeholder__line_inset-footer loads"></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!----><!--[--><!--]--><!--]--></div><!--]--><!--]--></div></div><!--]--><!--]--></div></div><div class="tm-page__sidebar"><!--[--><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__placeholder_initial"></div><div class="tm-sexy-sidebar_initial tm-sexy-sidebar" style="margin-top:0px;"><!--[--><!--]--><!----><div class="tm-layout-sidebar__ads_initial tm-layout-sidebar__ads"><div class="banner-wrapper half-page tm-layout-sidebar__banner tm-layout-sidebar__banner_top" style="--d33b7e1a:600px;--411af8d8:auto;" data-v-0fffe3b8><!--[--><div class="placeholder-wrapper placeholder" data-v-0fffe3b8><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div class="adfox-banner-placeholder half-page" data-v-12f7bcca><div class="image loads" data-v-12f7bcca></div><div class="lines" data-v-12f7bcca><div class="line loads" data-v-12f7bcca></div><div class="line loads" data-v-12f7bcca></div><div class="line loads" data-v-12f7bcca></div></div></div><!----><!----><!----></div><div id="adfox_164725680533065327" class="tm-adfox-banner" data-v-0fffe3b8></div><!--]--></div></div><!--[--><!----><div></div><!----><section class="tm-block tm-block tm-block_spacing-around block" data-v-b18ba87b><header class="tm-block__header tm-block__header"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title">Ближайшие события</h2><!--[--><!--]--></div><!----></header><!--[--><div class="tm-block__body tm-block__body"><!--[--><div class="placeholder-wrapper" data-v-b18ba87b><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><section class="tm-block tm-block tm-block_spacing-none" tabindex="-1" data-v-9caf0b82><!----><!--[--><div class="event-card-placeholder is-widget" data-v-9caf0b82><div class="image loads" data-v-9caf0b82></div><div class="info" data-v-9caf0b82><div class="date line" data-v-9caf0b82></div><div class="title line" data-v-9caf0b82></div><div class="places line" data-v-9caf0b82></div><div class="places line" data-v-9caf0b82></div></div><div class="footer widget" data-v-9caf0b82><div class="link line" data-v-9caf0b82></div><div class="categories" data-v-9caf0b82><!--[--><div class="category line" data-v-9caf0b82></div><div class="category line" data-v-9caf0b82></div><div class="category line" data-v-9caf0b82></div><!--]--></div></div></div><!--]--><!----></section><!----></div><!--]--></div><!--]--><!----></section><!--]--><div class="banner-wrapper medium-rectangle tm-layout-sidebar__banner tm-layout-sidebar__banner_bottom" style="--d33b7e1a:250px;--411af8d8:auto;" data-v-0fffe3b8><!--[--><div class="placeholder-wrapper placeholder" data-v-0fffe3b8><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div class="adfox-banner-placeholder medium-rectangle" data-v-12f7bcca><div class="image loads" data-v-12f7bcca></div><div class="lines" data-v-12f7bcca><div class="line loads" data-v-12f7bcca></div><div class="line loads" data-v-12f7bcca></div><div class="line loads" data-v-12f7bcca></div></div></div><!----><!----><!----></div><div id="adfox_164725691003361602" class="tm-adfox-banner" data-v-0fffe3b8></div><!--]--></div></div></div><!--]--></div></div><!----><!--]--></div></div></main><!----></div><div class="tm-footer-menu"><div class="tm-page-width"><!--[--><div class="tm-footer-menu__container"><!--[--><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">Ваш аккаунт</p><div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><!--[--><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/articles/921540/&amp;hl=ru" rel="nofollow" target="_self">Войти</a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/articles/921540/&amp;hl=ru" rel="nofollow" target="_self">Регистрация</a></li><!--]--></ul></div></div><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">Разделы</p><div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><!--[--><li class="tm-footer-menu__list-item"><a href="/ru/articles/" class="footer-menu__item-link">Статьи</a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">Новости</a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">Хабы</a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">Компании</a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">Авторы</a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">Песочница</a></li><!--]--></ul></div></div><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">Информация</p><div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><!--[--><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">Устройство сайта</a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">Для авторов</a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">Для компаний</a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">Документы</a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement/?hl=ru_RU" target="_blank">Соглашение</a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/?hl=ru_RU" target="_blank">Конфиденциальность</a></li><!--]--></ul></div></div><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">Услуги</p><div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><!--[--><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/corporate-blogs/" target="_blank">Корпоративный блог</a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/advertising/" target="_blank">Медийная реклама</a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/native-special/" target="_blank">Нативные проекты</a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/education-programs/" target="_blank">Образовательные программы</a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/hello-startup/" target="_blank">Стартапам</a></li><!--]--></ul></div></div><!--]--></div><!--]--></div></div><div class="tm-footer"><div class="tm-page-width"><!--[--><div class="tm-footer__container"><!----><div class="tm-footer__social"><!--[--><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Facebook</title><use xlink:href="/img/new-social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Twitter</title><use xlink:href="/img/new-social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>VK</title><use xlink:href="/img/new-social-icons-sprite.svg#social-logo-vk"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Telegram</title><use xlink:href="/img/new-social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Youtube</title><use xlink:href="/img/new-social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://dzen.ru/habr" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Яндекс Дзен</title><use xlink:href="/img/new-social-icons-sprite.svg#social-logo-dzen"></use></svg></a><!--]--></div><!--teleport start--><!--teleport end--><button class="tm-footer__link"><!----> Настройка языка</button><a href="/ru/feedback/" class="tm-footer__link">Техническая поддержка</a><div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2025, </span><span class="tm-copyright__name"><a class="tm-copyright__link" href="https://company.habr.com/" rel="noopener" target="_blank">Habr</a></span></span></div></div><!--]--></div></div><!----><!--]--></div><!----></div><script>window.__PINIA_STATE__={"i18n":{"fl":"ru","hl":"ru"},"feature":{"isProbablyVisible":false},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"query":{"code":"7bf2ed7d9e48b9839b3a8ce0369c369e","state":"Q8x6dDfN5tUynx0CAoDAvi44","hl":"ru"},"pathname":"\u002Fru\u002Farticles\u002F921540\u002F","path":"\u002Fru\u002Farticles\u002F921540\u002F","href":"\u002Fru\u002Farticles\u002F921540\u002F?code=7bf2ed7d9e48b9839b3a8ce0369c369e&state=Q8x6dDfN5tUynx0CAoDAvi44&hl=ru"}},"global":{"isPwa":false,"device":"desktop","isHabrCom":true,"requestId":"2dc51fc1f9ca310013d51e039d609683"},"articlesList":{"articlesList":{"921540":{"id":"921540","timePublished":"2025-06-29T08:46:58+00:00","isCorporative":false,"lang":"ru","titleHtml":"Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)","leadData":{"textHtml":"\u003Cp\u003EРелиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы она обучалась в стандартных f16, её вес был бы 1400гб, а мы попробуем версию в 10 раз меньше. Запустим самый маленький 1.66-битный IQ1_S_R4 квант полноценной модели размером 130гб на игровом ПК, отдельно с 4090 и 4060ti. Загрузим туда очень-очень много контекста и проверим, такой квант всё ещё способен давать разумные ответы или нет.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F21c\u002F125\u002F26f\u002F21c12526f6b5a79b4350d686a03883e8.png","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F21c\u002F125\u002F26f\u002F21c12526f6b5a79b4350d686a03883e8.png","fit":"cover","positionY":0,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[],"author":{"id":"94844","alias":"Shannon","fullname":null,"avatarUrl":null,"speciality":null,"scoreStats":{"score":146,"votesCount":176},"rating":56.6,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"donationsMethod":null,"isInBlacklist":null,"careerProfile":null},"statistics":{"commentsCount":39,"favoritesCount":149,"readingCount":20965,"score":109,"votesCount":89,"votesCountPlus":89,"votesCountMinus":0},"hubs":[{"id":"21922","alias":"artificial_intelligence","type":"collective","title":"Искусственный интеллект","titleHtml":"Искусственный интеллект","isProfiled":false,"relatedData":null}],"flows":[{"id":"7","alias":"popsci","title":"Научпоп","titleHtml":"Научпоп"}],"relatedData":{"vote":null,"unreadCommentsCount":0,"bookmarked":false,"canComment":false,"canEdit":false,"canViewVotes":false,"votePlus":{"canVote":false,"isChargeEnough":false,"isKarmaEnough":false,"isVotingOver":false,"isPublicationLimitEnough":false},"voteMinus":{"canVote":false,"isChargeEnough":false,"isKarmaEnough":false,"isVotingOver":false,"isPublicationLimitEnough":false},"canModerateComments":false,"trackerSubscribed":false,"emailSubscribed":false},"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F37b\u002F790\u002F19b\u002F37b79019ba325f57f446bac66156b1e5.png\" width=\"1139\" height=\"650\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F37b\u002F790\u002F19b\u002F37b79019ba325f57f446bac66156b1e5.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F37b\u002F790\u002F19b\u002F37b79019ba325f57f446bac66156b1e5.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EРелиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы она обучалась в стандартных f16, её вес был бы 1400гб, а мы попробуем версию в 10 раз меньше. Запустим самый маленький 1.66-битный IQ1_S_R4 квант полноценной модели размером 130гб на игровом ПК, отдельно с 4090 и 4060ti. Загрузим туда очень-очень много контекста и проверим, такой квант всё ещё способен давать разумные ответы или нет.\u003C\u002Fp\u003E\u003Ch2\u003EКак запускать\u003C\u002Fh2\u003E\u003Cp\u003EПочти все массовые локальные модели запускаются одинаковым образом, есть движок llama.cpp и формат этого движка gguf, с различными вариантами квантования, и есть оболочки, которые под капотом запускают тот самый llama.cpp - это и ollama, и LM Studio и все остальные.\u003C\u002Fp\u003E\u003Cp\u003EЧтобы просто запустить локально любую небольшую модель, достаточно скачать \u003Ca href=\"https:\u002F\u002Flmstudio.ai\u002F\" rel=\"noopener noreferrer nofollow\"\u003ELM Studio\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fjan.ai\u002F\" rel=\"noopener noreferrer nofollow\"\u003EJan\u003C\u002Fa\u003E или, если нужен более гибкий функционал, \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Foobabooga\u002Ftext-generation-webui\" rel=\"noopener noreferrer nofollow\"\u003Etext-generation-webui\u003C\u002Fa\u003E, потом любым способом скачать gguf файл и запустить всё в пару кликов. Это будет работать локально и займет несколько минут на разобраться. Всё это работает и на nvidia, и на amd, и на intel, там где нет CUDA, отлично работает через Vulkan. \u003C\u002Fp\u003E\u003Cp\u003EНо сегодня нас интересует кое-что посложнее, запустить настоящую большую DeepSeek R1-0528 размером 671B на домашнем игровом ПК. Это запуск не на б\u002Fу сервере, не на каком-то специфичном дорогом железе, не на куче видеопамяти, а на обычном ПК.\u003C\u002Fp\u003E\u003Cp\u003EЗапускать будем не обычное квантование вроде Q4_K_M или IQ1_S, и не динамическое квантование UD-...-XL, которое превосходит обычные кванты. Нас интересует sota квантование iq4_ks и R4, которое работает только в ik_llama.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\" rel=\"noopener noreferrer nofollow\"\u003Eik_llama.cpp\u003C\u002Fa\u003E - это форк от llama.cpp, который улучшает производительно на CPU и имеет расширенную поддержку MoE моделей, а так же является создателем передовых новых квантов. Именно через iq4_ks и R4 стало возможно создать настолько маленький квант, который ещё может показывает адекватные результаты и влезает в домашний ПК.\u003C\u002Fp\u003E\u003Ch3\u003EНа чём запускать\u003C\u002Fh3\u003E\u003Cp\u003EНам нужно много памяти, минимум можно попробовать 128гб, сейчас комплект из 4х модулей памяти 48гб DDR5 для домашних ПК стоит в пределах 50к, DDR4 4x32гб в 2 раза дешевле, и в продаже также начали появляться недорогие модули 2x64гб. Можно сказать, что 128\u002F192гб ram это уже вполне доступное железо.\u003C\u002Fp\u003E\u003Cp\u003EСами характеристики ПК не так важны, если там есть 6-8 ядер, важнее объем памяти и наличие 1 GPU, что является ключевым фактором для ускорения. Когда используются именно 4 модуля по 48гб, они плохо держат разгон и не стартуют на XMP, но хватит и того, что они запускаются на базовой частоте 4800.\u003C\u002Fp\u003E\u003Cp\u003EХарактеристики испытуемого ПК: \u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003ECPU: i7-14700\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EМатеринка: GIGABYTE Z790 D AX\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EОЗУ: 4x 48gb Kingbank DDR5 4800 MT\u002Fs\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EGPU: 4060 Ti 16gb, 4090 24gb\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EСравним отдельно 4090 и 4060 ti (хотя сейчас уже актуальнее 5060 Ti 16гб, у неё в 1.5 раза быстрее память чем у 4060ti, а стоит столько же), чтобы понять влияние GPU на скорость. \u003C\u002Fp\u003E\u003Ch2\u003EЧто запускать? Какой квант?\u003C\u002Fh2\u003E\u003Ch4\u003EDeepSeek-R1-0528-IQ1_S_R4 и DeepSeek-V3-0324-IQ1_S_R4\u003C\u002Fh4\u003E\u003Cp\u003EНас интересует репозиторий \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fubergarm\u002FDeepSeek-R1-0528-GGUF\u002F\" rel=\"noopener noreferrer nofollow\"\u003Eubergarm\u002FDeepSeek-R1-0528-GGUF\u003C\u002Fa\u003E - пока единственный кто предоставляет готовые кванты для ik_llama. Среди них нам нужен самый маленький размером 130гб - это IQ1_S_R4. \u003C\u002Fp\u003E\u003Cp\u003EЕсли не хотите ждать долгих рассуждений от R1, то можно взять V3-0324, для него тоже есть такой квант: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fubergarm\u002FDeepSeek-V3-0324-GGUF\u002Ftree\u002Fmain\u002FDeepSeek-V3-0324-IQ1_S_R4\" rel=\"noopener noreferrer nofollow\"\u003EDeepSeek-V3-0324-IQ1_S_R4\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003EКвант экстремально малого размера, и замеры качества через PPL показывают, что он ощутимо отстает от оригинала.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fac2\u002Fb64\u002Fed0\u002Fac2b64ed0aaf4bf31a2fb9027d8003c5.png\" alt=\"Чем ниже PPL, тем лучше\" title=\"Чем ниже PPL, тем лучше\" width=\"780\" height=\"256\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fac2\u002Fb64\u002Fed0\u002Fac2b64ed0aaf4bf31a2fb9027d8003c5.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fac2\u002Fb64\u002Fed0\u002Fac2b64ed0aaf4bf31a2fb9027d8003c5.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EЧем ниже PPL, тем лучше\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНо надежда есть, так как PPL не лучший показатель, он не отображает реальное качество кванта, и замеры KLD намного лучше отображают как квант далек от оригинала.\u003C\u002Fp\u003E\u003Cp\u003EВот сравнение 3 малых квантов: от Bartowski, Unsloth и Ubergarm, каждый со своей версией минимального размера. Все показатели чем ниже, тем лучше. Квант R4 имея самый маленький размер, показывает что обладает каким-то качеством:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa43\u002Fe07\u002F23e\u002Fa43e0723ec398948f69e9fb762c3c068.png\" alt=\"KLD отклонения от Q8_0, чем ниже, тем лучше\" title=\"KLD отклонения от Q8_0, чем ниже, тем лучше\" width=\"1782\" height=\"1160\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa43\u002Fe07\u002F23e\u002Fa43e0723ec398948f69e9fb762c3c068.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa43\u002Fe07\u002F23e\u002Fa43e0723ec398948f69e9fb762c3c068.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EKLD отклонения от Q8_0, чем ниже, тем лучше\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cdetails class=\"spoiler\"\u003E\u003Csummary\u003EПро разницу PPL и KLD\u003C\u002Fsummary\u003E\u003Cdiv class=\"spoiler__content\"\u003E\u003Cp\u003EВ работе Accuracy is Not All You Need (\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2407.09141\" rel=\"noopener noreferrer nofollow\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F2407.09141\u003C\u002Fa\u003E) показали, что KLD лучше отображает корреляцию между ошибками квантования и метрикой KLD, чем PPL, так как PPL скрывает ошибки квантования из-за усреднения.\u003C\u002Fp\u003E\u003Cp\u003EPPL (Perplexity) - это степень неуверенности модели в предсказании токена, чем ниже, тем увереннее модель. PPL усредняет логарифмические вероятности по всем токенам, поэтому ошибки, например, завышение вероятности одних токенов и занижение других, могут компенсировать друг друга - в результате PPL близок к оригиналу, хотя результат искажен. Ещё PPL слабо реагирует на ошибки в редких токенах, важных для генерации разнообразных ответов.\u003C\u002Fp\u003E\u003Cp\u003EKLD (KL Divergence) измеряет расхождение между распределениями исходной и квантованной моделей для каждого токена, потом суммирует расхождения для всех токенов. Тут ошибки никак не компенсируются друг другом, отклонения в вероятностях редких и частых токенов одинаково повлияют на итог. Это куда лучше позволяет оценить потери при квантовании, и если оптимизировать квантование под минимизацию KLD, то в среднем это улучшает кванты.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdetails\u003E\u003Ch2\u003EЗамеры скорости памяти\u003C\u002Fh2\u003E\u003Cp\u003EСкорость памяти прямо пропорциональна скорости генерации.\u003C\u002Fp\u003E\u003Cp\u003E2 модуля памяти, даже модули 48гб, обычно хорошо разгоняются и держать XMP 6400, на такой частоте можно получить почти 100гб\u002Fс. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5ae\u002F612\u002F881\u002F5ae612881bf3c9c8321524e7b4ba5ce6.png\" width=\"539\" height=\"518\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5ae\u002F612\u002F881\u002F5ae612881bf3c9c8321524e7b4ba5ce6.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5ae\u002F612\u002F881\u002F5ae612881bf3c9c8321524e7b4ba5ce6.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНо так как нам нужно больше памяти, то замерим скорости работы на 4х модулях. Скорость чтения на 4x DDR5-4800 равна 70 Гб\u002Fс, это не очень быстро, это ближе к скорости хорошей DDR4, чем к DDR5-6400, но этого должно хватить.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4fa\u002F63b\u002F483\u002F4fa63b48328fe5520c8d3e3bfcd2d762.png\" width=\"539\" height=\"518\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4fa\u002F63b\u002F483\u002F4fa63b48328fe5520c8d3e3bfcd2d762.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4fa\u002F63b\u002F483\u002F4fa63b48328fe5520c8d3e3bfcd2d762.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПроверим скорость Gemma3, генерация только на CPU, в стандартном кванте Q4_K_M. Запуск обычной llama.cpp на Windows 10, планировщик не оптимизирован на работу с малыми и большими ядрами:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\\llama-bench -m \"gemma-3-12b-it-Q4_K_M.gguf\" -t 4 -t 6 -t 8 -t 20 -t 28\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F50e\u002Fba9\u002Fd1a\u002F50eba9d1a83031a1cf402184bafb1cf1.png\" alt=\"Gemma3 12B Q4_K_M\" title=\"Gemma3 12B Q4_K_M\" width=\"947\" height=\"545\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F50e\u002Fba9\u002Fd1a\u002F50eba9d1a83031a1cf402184bafb1cf1.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F50e\u002Fba9\u002Fd1a\u002F50eba9d1a83031a1cf402184bafb1cf1.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EGemma3 12B Q4_K_M\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Ccode\u003E.\\llama-bench -m \"gemma-3-27b-it-Q4_K_M.gguf\" -t 4 -t 6 -t 8 -t 20 -t 28\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbb5\u002F2da\u002Ffc0\u002Fbb52dafc048b82e76591d95064ed899c.png\" alt=\"Gemma3 27B Q4_K_M\" title=\"Gemma3 27B Q4_K_M\" width=\"942\" height=\"573\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbb5\u002F2da\u002Ffc0\u002Fbb52dafc048b82e76591d95064ed899c.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbb5\u002F2da\u002Ffc0\u002Fbb52dafc048b82e76591d95064ed899c.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EGemma3 27B Q4_K_M\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003Epp - это promp processing, он же prefill. В pp входит системный промпт и вся история диалога. До тех пор пока контекст не закэширован, всё будет считаться от самого начала. \u003C\u002Fp\u003E\u003Cp\u003Etg - это token generation, генерация новых токенов, обычно все обращают внимание только на этот показатель, но на огромном контексте pp будет так же важен.\u003C\u002Fp\u003E\u003Ch2\u003EКак запускать R1 671B на одной GPU и за счёт чего ускорение\u003C\u002Fh2\u003E\u003Cp\u003EПамять не очень быстрая, даже Gemma3 12B весом 7гб еле выходит за границу комфортности, которая составляет 5 t\u002Fs. В таких условия нам нужно запустить квант весом 130гб имея для ускорения всего 1 GPU и на сколько это вообще возможно.\u003C\u002Fp\u003E\u003Cp\u003EGemma3 это dense-модель, то есть сплошная, для каждого нового токена нужно обойти все параметры модели. DeepSeek V3\u002FR1 - это MoE модель, где на каждом шагу использует только часть параметров.\u003C\u002Fp\u003E\u003Cp\u003EMoE это сокращение архитектуры Mixture of Experts, в таких моделях количество параметров (B) всей модели больше чем количество активных параметров (AxB) необходимых для каждого нового токена. Например, модель Qwen3-235B-A22B имеет всего 235B параметров и  на каждом шагу из них только 22B будут активными.\u003C\u002Fp\u003E\u003Cp\u003EУ R1 количество параметров 671B, активных параметров 37B. Всего 61 слой, 3 слоя общих, которые используются на каждом шагу, остальные слои экспертов, они выбираются роутером на каждом шагу разные, поэтому нельзя просто загрузить 37B в vram.\u003C\u002Fp\u003E\u003Cp\u003EЕсли модель целиком влезает в память, то скорость инференса будет примерно равна dense-модели размером 37B, а именно в районе 2 t\u002Fs. Это генерация на такой скорости памяти, была бы память быстрее, то и генерация была бы быстрее. 2 t\u002Fs мало, нужна помощь от GPU, но если просто выгрузить 10 слоев в vram, то будет ситуация, когда только первые 3 слоя полезны, остальные будут выпадать лишь иногда.\u003C\u002Fp\u003E\u003Cp\u003EВыгрузив часть слоев ускорение есть, но совсем не существенное, нужно больше:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4b0\u002F7e3\u002Ff18\u002F4b07e3f182c2fade910d5eb1ea8f547e.png\" alt=\"\" title=\"\" width=\"645\" height=\"100\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4b0\u002F7e3\u002Ff18\u002F4b07e3f182c2fade910d5eb1ea8f547e.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4b0\u002F7e3\u002Ff18\u002F4b07e3f182c2fade910d5eb1ea8f547e.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EРешение: \u003Cem\u003Eoverride-tensor\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\u003Cp\u003EЧтобы получить существенное ускорение, нужно чтобы GPU полноценно участвовала в работе на каждом шагу и решение тут в том, чтобы на GPU выгрузить только веса внимания, которые легкие и на каждом шагу важны. А экспертные ffn каждого слоя оставить на CPU. \u003C\u002Fp\u003E\u003Cp\u003EВ llm трансформерах модель состоит из слоев, каждый слой состоит из 2х видов тензоров: внимания (attn, attention) и полносвязной сети (ffn, feed forward network). Посмотреть структуру модели можно на huggingface если нажать на конкретный квант. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F400\u002Fce3\u002F24f\u002F400ce324f0f32ea1672463ad415d2430.png\" alt=\"18-й слой deepseek r1, самые тяжелые тензоры это ffn, а важные attn намного легче\" title=\"18-й слой deepseek r1, самые тяжелые тензоры это ffn, а важные attn намного легче\" width=\"692\" height=\"475\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F400\u002Fce3\u002F24f\u002F400ce324f0f32ea1672463ad415d2430.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F400\u002Fce3\u002F24f\u002F400ce324f0f32ea1672463ad415d2430.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E18-й слой deepseek r1, самые тяжелые тензоры это ffn, а важные attn намного легче\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EТензоры внимания весят не очень много, и в таком низком кванте они влезают даже в 10гб видеопамяти, оставляя достаточно места для контекста, а ffn экспертов и составляют основной объем модели.\u003C\u002Fp\u003E\u003Ch2\u003E--override-tensor или -ot\u003C\u002Fh2\u003E\u003Cp\u003EДля того, чтобы разнести разбить слой на тензоры в llama.cpp и ik_llama добавили параметр \u003Ccode\u003E-ot\u003C\u002Fcode\u003E или \u003Ccode\u003E--override-tensors\u003C\u002Fcode\u003E , через него указывают какие тензоры отправятся на CPU (или другие устройства, например, вторую GPU) используя regexp синтаксис.\u003C\u002Fp\u003E\u003Cp\u003EНужно выгрузить все легковесные тензоры на GPU, а тяжелые, которые упрощенно называют MoE-параметрами, на CPU. Чтобы это сделать, нужно сначала выгрузить все слои на видеокарту через параметр \u003Ccode\u003E-ngl 999\u003C\u002Fcode\u003E , а потом указываем какие надо перенаправить в обычную память.\u003C\u002Fp\u003E\u003Cp\u003EMoE-параметры это те, которые имеют в имени exps, то есть эксперты, поэтому нужно просто указать один из синонимов, который выберет всех exps:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ot exps=CPU\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ot \".ffn_.*_exps.=CPU\"\u003C\u002Fcode\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ot \"([0-9]+).ffn_.*_exps.=CPU\"\u003C\u002Fcode\u003E \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F962\u002Fefe\u002Fedd\u002F962efeedd5c67dee42c5c6a2e46f6695.png\" alt=\"Точные имена тензоров, чтобы составить правильный regexp\" title=\"Точные имена тензоров, чтобы составить правильный regexp\" width=\"702\" height=\"478\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F962\u002Fefe\u002Fedd\u002F962efeedd5c67dee42c5c6a2e46f6695.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F962\u002Fefe\u002Fedd\u002F962efeedd5c67dee42c5c6a2e46f6695.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EТочные имена тензоров, чтобы составить правильный regexp\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EТеперь на видеокарте остались какие-то тензоры каждого слоя, и gpu на каждом шаге будет участвовать в генерации, за счет этого и происходит ускорение. И это в основном подходит только для MoE моделей.\u003C\u002Fp\u003E\u003Cp\u003EИ, обычно, если осталась свободная VRAM, или не нужно так много контекста, или есть вторая видеокарта, то можно больше тензоров оставить выгруженными на видеокарте. \u003C\u002Fp\u003E\u003Cp\u003E \u003Ccode\u003E-ot \"blk\\.([4-9])\\.ffn.*=CUDA0\" -ot \"blk\\.(1[0-5])\\.ffn.*=CUDA1\" -ot exps=CPU\u003C\u002Fcode\u003E  \u003C\u002Fp\u003E\u003Cp\u003EТак мы оставим 4-9 слои целиком на 1 GPU и 10-15 слои на 2 GPU. Для rocm или vulkan будут свои синонимы названия устройств, вроде Vulkan0 вместо CUDA0.\u003C\u002Fp\u003E\u003Cp\u003EВ кванте IQ1_S_R4 для тензоров ffn up|gate|down использованы тензоры R4, которые оптимизированы для работы на CPU, их нужно автоматически конвертировать в IQK квант пригодные для GPU. Для этого ik_llama надо скомпилировать с флагом \u003Ccode\u003E\u003Cem\u003E-DGGML_CUDA_IQK_FORCE_BF16=1\u003C\u002Fem\u003E\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Ch2\u003EЗапускаем DeepSeek R1 671B IQ1_S_R4\u003C\u002Fh2\u003E\u003Cp\u003EУ \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\" rel=\"noopener noreferrer nofollow\"\u003Eik_llama\u003C\u002Fa\u003E нет готовых бинарников, как у \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fggml-org\u002Fllama.cpp\u002Freleases\" rel=\"noopener noreferrer nofollow\"\u003Ellama.cpp\u003C\u002Fa\u003E, поэтому нужно будет собрать её из исходников, делается это по той же инструкции как и llama.cpp, поэтому сложности не должно возникнуть. \u003C\u002Fp\u003E\u003Cp\u003EЕсли одна GPU. Вместо \u003Ccode\u003E-j28\u003C\u002Fcode\u003E укажите ваше количество ядер или потоков:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"bash\"\u003Egit clone https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\ncd ik_llama\ncmake -B .\u002Fbuild -DGGML_CUDA=ON -DGGML_BLAS=OFF\ncmake --build build --config Release -j28\ncd build\u002Fbin\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EЕсли планируется использовать несколько GPU:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"bash\"\u003Egit clone https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\ncd ik_llama\ncmake -B .\u002Fbuild -DGGML_CUDA=ON -DGGML_BLAS=OFF -DGGML_SCHED_MAX_COPIES=1 -DGGML_CUDA_IQK_FORCE_BF16=1\ncmake --build build --config Release -j28\ncd build\u002Fbin\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EЕсли не указать \u003Ccode\u003E-DGGML_SCHED_MAX_COPIES=1\u003C\u002Fcode\u003E, то будет перерасход видеопамяти с использованием \u003Ccode\u003E-ot\u003C\u002Fcode\u003E. \u003Ccode\u003E-DGGML_CUDA_IQK_FORCE_BF16=1\u003C\u002Fcode\u003E нужен для выгрузки на GPU ffn тензоров, но не всегда дает ускорение, иногда замедляет работу, зависит от модели видеокарты.\u003C\u002Fp\u003E\u003Cp\u003EПосле этого можно запустить llama-server:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-server -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -ctk q8_0 -amb 512 -fmoe -ot exps=CPU -ngl 99 -b 4096 -ub 4096 -t 20 -c 8192 \u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003EПо адресу \u003Ca href=\"http:\u002F\u002F127.0.0.1:8080\u002F\" rel=\"noopener noreferrer nofollow\"\u003Ehttp:\u002F\u002F127.0.0.1:8080\u002F\u003C\u002Fa\u003E будет доступен вполне удобный веб-клиент:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a0\u002Fd22\u002F34d\u002F8a0d2234d5015348adf430e72241917f.png\" alt=\"2184 токенов размышления, модель долго искала подвох, ответ 87 токенов. Скорость 7 t\u002Fs\" title=\"2184 токенов размышления, модель долго искала подвох, ответ 87 токенов. Скорость 7 t\u002Fs\" width=\"1068\" height=\"511\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a0\u002Fd22\u002F34d\u002F8a0d2234d5015348adf430e72241917f.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a0\u002Fd22\u002F34d\u002F8a0d2234d5015348adf430e72241917f.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E2184 токенов размышления, модель долго искала подвох, ответ 87 токенов. Скорость 7 t\u002Fs\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EМы получили 7 t\u002Fs, это в 2 раза выше, чем мы получали когда вынесли только начальные слои на GPU, хотя в обоих случая количество занимаемой памяти подобрано одинаково, что показывает, что подход через \u003Ccode\u003E-ot\u003C\u002Fcode\u003E работает.\u003C\u002Fp\u003E\u003Cp\u003EПодробнее про параметры запуска:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-m\u003C\u002Fcode\u003E - путь до файла модели, у huggingface есть ограничение на размер файла 50гб, поэтому файлов будет несколько разбитых по шаблону 00001-0000x.gguf. Для запуска нужно указывать только 1 файл.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-fa\u003C\u002Fcode\u003E - flash attention, способ эффективнее считать контекст, тратя меньше памяти, при правильной реализации математически идентичен обычному attention.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-amb 512\u003C\u002Fcode\u003E - переиспользовать \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\u002Fpull\u002F237\" rel=\"noopener noreferrer nofollow\"\u003Eбуфер для вычисления K*Q\u003C\u002Fa\u003E, размер в mb, можно увеличить, если хватает памяти.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-fmoe\u003C\u002Fcode\u003E - \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\u002Fpull\u002F229\" rel=\"noopener noreferrer nofollow\"\u003Efused moe\u003C\u002Fa\u003E, объединяет up, gate и act операции, немного ускоряя вычисления.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-mla 3\u003C\u002Fcode\u003E - включить mla\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ctk q8_0\u003C\u002Fcode\u003E - квантование только k-кэша контекста, считается, что он почти не страдает от квантования, в отличии от v-кэша.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ngl 99\u003C\u002Fcode\u003E - выгрузить все слои на GPU.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ot exps=CPU\u003C\u002Fcode\u003E - отправить все тензоры где в имени exps на CPU.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-b 4096 -ub 4096\u003C\u002Fcode\u003E - оптимизация размеров батчей, может ускорить вычисление pp.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-t 20\u003C\u002Fcode\u003E - использовать все ядра, что есть, по умолчанию 8.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-c 8192\u003C\u002Fcode\u003E - задать размер контекста 8к, по умолчанию 4к.\u003C\u002Fp\u003E\u003Cp\u003EДополнительные параметры:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-rtr\u003C\u002Fcode\u003E - если запуск CPU only, то при загрузке конвертировать веса в оптимизированные для работы на CPU, отключает mmap\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ser 6,1\u003C\u002Fcode\u003E - умное \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fikawrakow\u002Fik_llama.cpp\u002Fpull\u002F239\" rel=\"noopener noreferrer nofollow\"\u003Eуменьшение количества экспертов\u003C\u002Fa\u003E, ускоряет работу за счет небольшого снижения качества.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E-ts 24,16\u003C\u002Fcode\u003E - если установлены две gpu, то можно распределить по ним слои в заданной пропорции, полезно для dense-моделей, если используется -ot, то лучше не использовать.\u003C\u002Fp\u003E\u003Ch2\u003EБенчмарк скорости DeepSeek R1\u003C\u002Fh2\u003E\u003Cp\u003EТеперь можно заняться более точным измерением скорости генерации как новых токенов, так и уже существующего контекста. У ik_llama есть удобный инструмент для замера скоростей модели на указанной длине контекста. \u003C\u002Fp\u003E\u003Cp\u003EСравним 3 варианта, вначале на типичном контексте 4к, параметры -b и -ub по-умолчанию:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003ECPU only (скорость памяти 72 гб\u002Fс)\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EУскорение через 4060 ti 16гб (скорость памяти 288 гб\u002Fс)\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EУскорение через 4090 24гб (скорость памяти 1008 гб\u002Fс)\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EN_KV - размер контекста.\u003C\u002Fp\u003E\u003Cp\u003ET_PP - время генерации PP.\u003C\u002Fp\u003E\u003Cp\u003ES_PP - скорость генерации pp.\u003C\u002Fp\u003E\u003Cp\u003ES_TG - скорость генерации tg.\u003C\u002Fp\u003E\u003Cp\u003ECPU only, скрываем все CUDA устройства через \u003Ccode\u003ECUDA_VISIBLE_DEVICES=\"\"\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003ECUDA_VISIBLE_DEVICES=\"\" .\u002Fllama-sweep-bench -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ctk q8_0 -t 28 -c 4096\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F98e\u002F7fa\u002F0f1\u002F98e7fa0f16d46bbff8f5c35df40c34c2.png\" alt=\"\" title=\"\" width=\"934\" height=\"498\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F98e\u002F7fa\u002F0f1\u002F98e7fa0f16d46bbff8f5c35df40c34c2.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F98e\u002F7fa\u002F0f1\u002F98e7fa0f16d46bbff8f5c35df40c34c2.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E4060 ti:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003ECUDA_VISIBLE_DEVICES=\"1\" .\u002Fllama-sweep-bench -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 99 -ctk q8_0 -t 28 -c 4096\u003C\u002Fcode\u003E\u003Cbr\u002F\u003E \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5e9\u002Fc5b\u002F92b\u002F5e9c5b92ba6f2597a01a97766de228ed.png\" width=\"930\" height=\"447\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5e9\u002Fc5b\u002F92b\u002F5e9c5b92ba6f2597a01a97766de228ed.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5e9\u002Fc5b\u002F92b\u002F5e9c5b92ba6f2597a01a97766de228ed.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E4090:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003ECUDA_VISIBLE_DEVICES=\"0\" .\u002Fllama-sweep-bench -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 99 -ctk q8_0 -t 28 -c 4096\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F270\u002Fe54\u002F5e0\u002F270e545e068f018fbf81d72b1dca4ea7.png\" width=\"941\" height=\"450\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F270\u002Fe54\u002F5e0\u002F270e545e068f018fbf81d72b1dca4ea7.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F270\u002Fe54\u002F5e0\u002F270e545e068f018fbf81d72b1dca4ea7.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EРазница между 4060 ti и 4090 не соответствует разнице производительности и скорости памяти. Всё потому, что на GPU уходит всего ~10 гб тензоров и при маленьком батче разница между видеокартами не так заметна.\u003C\u002Fp\u003E\u003Cp\u003EТеперь замерим контекст 32к, увеличим размер батчей, параметры \u003Ccode\u003E-b 4096 -ub 4096\u003C\u002Fcode\u003E. Увеличивая размер батчей, мы увеличим скорость PP, что важно при работе с большим контекстом, когда обработка 100к может занимать почти час на 25 t\u002Fs.\u003C\u002Fp\u003E\u003Cp\u003E4060 ti:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F488\u002Ff02\u002F787\u002F488f02787c0235bf8bb1d24d4a0dc810.png\" width=\"932\" height=\"451\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F488\u002Ff02\u002F787\u002F488f02787c0235bf8bb1d24d4a0dc810.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F488\u002Ff02\u002F787\u002F488f02787c0235bf8bb1d24d4a0dc810.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E4090: \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fea8\u002F4ed\u002F39f\u002Fea84ed39faacf9cf33c20e575a09d127.png\" width=\"934\" height=\"443\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fea8\u002F4ed\u002F39f\u002Fea84ed39faacf9cf33c20e575a09d127.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fea8\u002F4ed\u002F39f\u002Fea84ed39faacf9cf33c20e575a09d127.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EТут разница на подготовку промпта между картами уже видна лучше, скорость памяти 1 Тб\u002Fс против 288 Гб\u002Fс - всё-таки разница большая. На скорости 1 Тб\u002Fс для такого небольшого объема данных уже и количество ядер может быть важным.\u003C\u002Fp\u003E\u003Cp\u003EНа 4090 скорость tg немного упала, это не критично, зато скорость pp выросла почти в 10 раз до 200-300 t\u002Fs. С такой скоростью уже можно обрабатывать огромные контексты. На 4060 ti pp вырос всего в 1.5-2 раза, это тоже не плохо, обработка 32к контекста займет 15 минут, а дальше она закэшируется и будет работать моментально. \u003C\u002Fp\u003E\u003Cp\u003EПараметром \u003Ccode\u003E-ser 6,1\u003C\u002Fcode\u003E можно компенсировать потери tg от \u003Ccode\u003E-ub 4096 -b 4096\u003C\u002Fcode\u003E, сохраняя 300 t\u002Fs на pp, и на коротком контексте возвращая 8 t\u002Fs на tg:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c0\u002F327\u002Fd63\u002F2c0327d6306ad6e00e3501b6adea0baa.png\" width=\"932\" height=\"198\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c0\u002F327\u002Fd63\u002F2c0327d6306ad6e00e3501b6adea0baa.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c0\u002F327\u002Fd63\u002F2c0327d6306ad6e00e3501b6adea0baa.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003EКак вместить огромный контекст в одну GPU?\u003C\u002Fh2\u003E\u003Cp\u003EКак получить ускорение через GPU разобрались. Теперь нужно разобраться как вместить огромный контекст в тот небольшой объем vram, который остаётся.\u003C\u002Fp\u003E\u003Cp\u003EУ DeepSeek R1 максимальный контекст 160к, это очень много, и в обычном виде на это требуется сотни гб памяти. Например, согласно исследованию \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2505.02390\" rel=\"noopener noreferrer nofollow\"\u003EQuantitative Analysis of Performance Drop in DeepSeek Model Quantization\u003C\u002Fa\u003E, для всего лишь 32к контекста нужно 400гб памяти на обычной llama.cpp.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5de\u002Fc67\u002F19b\u002F5dec6719bbf3e6f84797bda523430bd8.png\" width=\"904\" height=\"387\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5de\u002Fc67\u002F19b\u002F5dec6719bbf3e6f84797bda523430bd8.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5de\u002Fc67\u002F19b\u002F5dec6719bbf3e6f84797bda523430bd8.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EСогласно тому же исследованию, качество кванта \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Funsloth\u002FDeepSeek-R1-0528-GGUF\" rel=\"noopener noreferrer nofollow\"\u003EUD-Q2_K_XL\u003C\u002Fa\u003E (это динамическое квантование от Unsloth, главная особенность этого квантования в том, что важные тензоры оставлены в очень высоком качестве, а менее важные квантуются сильнее, за счет этого общее качество остается на высоком уровне) по сравнению с оригиналом FP8 падает всего на несколько процентов в бенчмарках:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe79\u002F021\u002F938\u002Fe7902193855bc67679ba2b46faf91439.png\" width=\"804\" height=\"805\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe79\u002F021\u002F938\u002Fe7902193855bc67679ba2b46faf91439.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe79\u002F021\u002F938\u002Fe7902193855bc67679ba2b46faf91439.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EТолько актуальная версия UD-Q2_K_XL занимает уже 233гб, а не 212гб, слишком много, но можно попробовать запустить её с ssd, это критично снизит скорость PP, но для средних контекстов всё еще может работать, например, 32к ждать пришлось минут 40:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F7b2\u002F91b\u002F0f3\u002F7b291b0f377f40e1fdf0172eaf59e75d.png\" alt=\"DeepSeek-R1-0528-UD-Q2_K_XL запуск частично с ssd, обработка 32к контекст, скорость 2.3 t\u002Fs\" title=\"DeepSeek-R1-0528-UD-Q2_K_XL запуск частично с ssd, обработка 32к контекст, скорость 2.3 t\u002Fs\" width=\"907\" height=\"724\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F7b2\u002F91b\u002F0f3\u002F7b291b0f377f40e1fdf0172eaf59e75d.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F7b2\u002F91b\u002F0f3\u002F7b291b0f377f40e1fdf0172eaf59e75d.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EDeepSeek-R1-0528-UD-Q2_K_XL запуск частично с ssd, обработка 32к контекст, скорость 2.3 t\u002Fs\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EКванты от ubergram по сути такое же динамическое квантование, только с использованием более продвинутых IQK и R4 квантов. \u003C\u002Fp\u003E\u003Cp\u003EНо возвращаясь к тому, как получилось, что мы уже и тестировали и запускали 32к и для этого не понадобилось 400гб памяти, как это работает?\u003C\u002Fp\u003E\u003Ch2\u003EMLA и Внимание\u003C\u002Fh2\u003E\u003Cp\u003EКонтекст так много весит, потому что он полностью связан, и постоянно динамически вычисляет важность токенов через механизм внимания. В отличии от таких статистических предсказателей следующего токена, вроде цепей Маркова, где предыдущее значение контекста не играет роли, играет только предыдущий токен, если это цепь 1 порядка, в трансформерах механизм внимания на каждом шагу работает со всей последовательностью, чтобы постоянно вычислять важность токенов и перепроверять, что ответ правильный. \u003C\u002Fp\u003E\u003Cp\u003EМеханизм Внимания - это квадратичная сложность \u003Ccode\u003EO(n²)\u003C\u002Fcode\u003E  и по памяти и по времени, где n-длина последовательности, каждый токен взаимодействует с каждым токеном, поэтому расход памяти так быстро растет. Чтобы справиться с этим, придумывают различные математические оптимизации, один из них это Flash Attention - используя математические трюки с матрицами уменьшается требования к памяти без потерь качества, результат идентичен обычному attention.\u003C\u002Fp\u003E\u003Cp\u003EИли другой подход - Sliding Window Attention (SWA), когда токену ограничивают область видимости, например каждый токен видит вокруг себя только вокруг себя в пределах окна, которое обычно 4096 токенов, за счёт этого можно обрабатывать очень длинные последовательности используя мало памяти, но взамен теряется информация вне окна, что можно частично компенсировать различными техниками.\u003C\u002Fp\u003E\u003Cp\u003EВ DeepSeek пошли другим путем, они попытались изменить сам подход к attention. Обычно для оптимизации внимания используют, например, Grouped Query Attention (GQA) или Multi-Query Attention (MQA), эти методы являются вычислительными оптимизациями стандартного механизма без фундаментального изменения архитектуры. Вместо такого подхода в DeepSeek разработали MLA (Multi-Head Latent Attention), где роль для внимания играет не токен, а латентный вектор. \u003C\u002Fp\u003E\u003Cp\u003EMLA - это обучение скрытых или латентных векторов вместе с основной моделью, эти вектора учатся улавливать ключевые концепции и паттерны в данных. Головы внимания в MLA взаимодействуют не напрямую с токенами, а с этими латентными векторами. За счет этого получается \"ужать\" KV-кэш в 25 раз сохраняя оригинальное качество:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F461\u002F59b\u002F548\u002F46159b548ef07b417b78f0ad44b9c600.png\" alt=\"MLA представили в DeepSeek V2: https:\u002F\u002Farxiv.org\u002Fabs\u002F2405.04434\" title=\"MLA представили в DeepSeek V2: https:\u002F\u002Farxiv.org\u002Fabs\u002F2405.04434\" width=\"863\" height=\"381\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F461\u002F59b\u002F548\u002F46159b548ef07b417b78f0ad44b9c600.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F461\u002F59b\u002F548\u002F46159b548ef07b417b78f0ad44b9c600.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EMLA представили в DeepSeek V2: \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2405.04434\" rel=\"noopener noreferrer nofollow\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F2405.04434\u003C\u002Fa\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EИ причина, почему в том исследовании им потребовалось 400гб для 32к контекста в том, что на момент исследования в llama.cpp не была реализована поддержка MLA.\u003C\u002Fp\u003E\u003Cp\u003EКроме MLA, у DeepSeek есть ещё одна интересная технология - MTP.\u003C\u002Fp\u003E\u003Cp\u003EMTP - это их реализация спекулятивного декодирования, способ переложить часть работы по предсказываю следующего токена на маленькую модель, это работает, когда продолжение слова или фразы уже очевидно. Если зайти на официальный репозиторий deepseek-ai, то размер модели будет 685B, а не 671B. Как раз 14B это \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2412.19437\" rel=\"noopener noreferrer nofollow\"\u003Eмодуль MTP\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffde\u002Fa2d\u002Fb7a\u002Ffdea2db7a50d436bbf2ef77e5af5eb75.png\" alt=\"Model size: 685B\" title=\"Model size: 685B\" width=\"1317\" height=\"503\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffde\u002Fa2d\u002Fb7a\u002Ffdea2db7a50d436bbf2ef77e5af5eb75.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffde\u002Fa2d\u002Fb7a\u002Ffdea2db7a50d436bbf2ef77e5af5eb75.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EModel size: 685B\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EMTP, в отличии от обычного спекулятивного декодирования, тоже обучалось вместе с модель. По их замерам точность принятия токенов от MTP 85-90%, что дает ускорение основной модели в 1.8 раза.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbed\u002F402\u002Feb8\u002Fbed402eb8dd4d567fd34377f9db28b88.png\" width=\"750\" height=\"253\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbed\u002F402\u002Feb8\u002Fbed402eb8dd4d567fd34377f9db28b88.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbed\u002F402\u002Feb8\u002Fbed402eb8dd4d567fd34377f9db28b88.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EВ llama.cpp сейчас есть реализация \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fggerganov\u002Fllama.cpp\u002Fpull\u002F10455\" rel=\"noopener noreferrer nofollow\"\u003Eспекулятивного декодирования\u003C\u002Fa\u003E, но она сделана по другому, там в качестве маленькой модели нужна полноценная модель того же семейства. Например, Gemma3 27B в качестве помощника может использовать только что-то из своих младших моделей, вроде Gemma3 1B.\u003C\u002Fp\u003E\u003Ch2\u003EСколько нужно памяти под контекст используя MLA\u003C\u002Fh2\u003E\u003Cp\u003EВ ik_llama, чтобы включить использование mla нужно добавить параметр \u003Ccode\u003E-fa -mla 3\u003C\u002Fcode\u003E . \u003C\u002Fp\u003E\u003Cp\u003EВ llama.cpp все виды внимания (mla, swa) используемые конкретной моделью включаются через \u003Ccode\u003E-fa\u003C\u002Fcode\u003E автоматически, если их поддержка уже добавлена в ядро.\u003C\u002Fp\u003E\u003Cp\u003EТензоры оставленные с -ot для кванта IQ1_S_R4 примерно равны 10.5гб, под нужны ОС, включая браузер, на 4090 уходит около 1.7гб. В итоге есть примерно 5гб и 11гб под контекст на 4060ti и 4090 соответственно. Перебором параметра \u003Ccode\u003E-c\u003C\u002Fcode\u003E можно найти количество контекста, которое влезает в этот объем. Чем больше размер батча, тем больше под них нужно памяти.\u003C\u002Fp\u003E\u003Cp\u003EРазмер батчей стандартный -b 2048 -ub 512:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb55\u002Fbc9\u002Fc75\u002Fb55bc9c750990ad4ea0064428d09f53a.png\" alt=\"Тензоры выгружены на GPU, b\u002Fub стандартные -b 2048 -ub 512\" title=\"Тензоры выгружены на GPU, b\u002Fub стандартные -b 2048 -ub 512\" width=\"934\" height=\"151\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb55\u002Fbc9\u002Fc75\u002Fb55bc9c750990ad4ea0064428d09f53a.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb55\u002Fbc9\u002Fc75\u002Fb55bc9c750990ad4ea0064428d09f53a.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EТензоры выгружены на GPU, b\u002Fub стандартные -b 2048 -ub 512\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EРазмер батчей -b 4096 -ub 4096:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F091\u002F0bc\u002F03c\u002F0910bc03ccb65ec1f3dbcae07bf5c761.png\" alt=\"Тензоры выгружены на GPU, под контекст остатки памяти, -b 4096 -ub 4096\" title=\"Тензоры выгружены на GPU, под контекст остатки памяти, -b 4096 -ub 4096\" width=\"940\" height=\"152\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F091\u002F0bc\u002F03c\u002F0910bc03ccb65ec1f3dbcae07bf5c761.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F091\u002F0bc\u002F03c\u002F0910bc03ccb65ec1f3dbcae07bf5c761.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EТензоры выгружены на GPU, под контекст остатки памяти, -b 4096 -ub 4096\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EКак вариант, можно не выгружать тензоры на GPU совсем, оставляя всю память под контекст. Для этого надо указать\u003Ccode\u003E-ngl 0\u003C\u002Fcode\u003E. Максимальные 160к без квантования требуют примерно 14гб при стандартном размере батчей и 18гб при 4096.\u003C\u002Fp\u003E\u003Cp\u003EЗапустим тот же бенчмарк, контекст 160к, все тензоры теперь на CPU:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003ECUDA_VISIBLE_DEVICES=\"0\" .\u002Fllama-sweep-bench -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 0 -ctk q8_0 -t 28 -c 163840 -ngl 31 -b 4096 -ub 4096\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F426\u002Fe5b\u002F144\u002F426e5b1443fa9a83a5fcdb7883504b66.png\" alt=\"\" title=\"\" width=\"943\" height=\"150\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F426\u002Fe5b\u002F144\u002F426e5b1443fa9a83a5fcdb7883504b66.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F426\u002Fe5b\u002F144\u002F426e5b1443fa9a83a5fcdb7883504b66.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EСкорость tg упала до скорости 2.85 t\u002Fs и соответствует \"CPU only\". Но с ростом контекста она быстро упадет до 1, а под конец и до 0.5 t\u002Fs. А скорость PP осталась как и была на GPU, около 200-300 t\u002Fs, но под конец 160к она упадёт до 75 t\u002Fs.\u003C\u002Fp\u003E\u003Cp\u003EТеперь проверим \"сложение\" двух GPU. Ожидание от двух карт такое, что можно разместить количество контекста сумме их отдельных размеров, то есть в 126к. Но на практике объединив две GPU можно уместить всего 110к с квантованием q8_0, что не совпадает с расчетами. Дело в том, что единственная польза от двух карт в том, что мы разделяем тензоры, отправляя 6гб на gpu1 и 3гб на gpu2, тем самым освобождая память для контекста, а сам контекст создается на двух GPU почти пропорционального размера, то есть не получится сложить 40к + 86к. Логика работы двух GPU и контекста мне пока не понятна.\u003C\u002Fp\u003E\u003Cp\u003EЗамер скорости двух GPU и контекста 110к:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-sweep-bench -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 99 -ts 20,10 -b 4096 -ub 4096 -ctk q8_0 -t 28 -c 112640\u003C\u002Fcode\u003E \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F803\u002Fd8d\u002Fdfb\u002F803d8ddfb7bcf5bc71659b95da363fa9.png\" alt=\"4090 + 4060ti, 110k контекста\" title=\"4090 + 4060ti, 110k контекста\" width=\"666\" height=\"242\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F803\u002Fd8d\u002Fdfb\u002F803d8ddfb7bcf5bc71659b95da363fa9.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F803\u002Fd8d\u002Fdfb\u002F803d8ddfb7bcf5bc71659b95da363fa9.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E4090 + 4060ti, 110k контекста\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EСкорости pp просели из-за медленной 4060, но всё еще пригодны для использования и не заставляют ждать слишком долго.\u003C\u002Fp\u003E\u003Ch2\u003EЗагружаем модель и проводим тесты\u003C\u002Fh2\u003E\u003Cp\u003EТеперь когда теории достаточно, как ускориться и как вместить огромный контекст понятно, осталось придумать как его проверить. Кодовой базы на 100к у меня нет, а работать с чужой - сложно оценить результат.\u003C\u002Fp\u003E\u003Cp\u003EЕщё один из вариантов проверить такой большой контекст - это взять знакомую книгу и протестировать модель на ней, сможет ли модель пересказать всю книгу, выдать какие-то факты из начала, середины и конца, и в целом по ответам будет понятно, модель способна обработать такой контекст или она просто выбирает случайные подходящие слова и выдает какой-то не особо связный ответ. Часто маленькие модели (7b, 12b, 14b) уже на контексте в 32к зацикливаются и просто выдают бесконечно одинаковый токен.\u003C\u002Fp\u003E\u003Cp\u003EНужна книга, которой точно нет в обучающем датасете, например, Лабиринт Отражений. Текст книги нужно целиком вставить в системный промпт, тогда это будет контекст модели, а потом позадавать вопросы по нему. Это будет не RAG, который разбивает текст на кусочки, создаёт векторную БД и разбивает данные на кусочки, и по ключевым словам достает эти кусочки и подмешивает их в контекст, такие кусочки не связаны друг с другом единым вниманием, поэтому такой подход подходит для документации, но не для связной книги. И это не какой-то похожий подход, это будет чистый контекст. \u003C\u002Fp\u003E\u003Cp\u003EПроверка модели с пустым системным промптом в своем \"базовом\" виде. Она ничего не знает о Лабиринте Отражений, но что-то знает про цикл Дозоры, что совсем не помогает.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe88\u002F845\u002Fad9\u002Fe88845ad9fb379e34bcca7c41548d68f.png\" alt=\"Пустой системный промпт\" title=\"Пустой системный промпт\" width=\"1071\" height=\"600\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe88\u002F845\u002Fad9\u002Fe88845ad9fb379e34bcca7c41548d68f.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe88\u002F845\u002Fad9\u002Fe88845ad9fb379e34bcca7c41548d68f.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EПустой системный промпт\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EСтоит подсчитать количество токенов из которого состоит книга, для этого можно воспользоваться утилитой llama-tokenize, и нужно указать модель, так как у всех моделей разные токенизаторы.\u003C\u002Fp\u003E\u003Cp\u003Elinux:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-tokenize -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -f book.txt | wc -l\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003Ewindows powershell:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\\llama-tokenize.exe -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -f book.txt | Measure-Object -Line | Select-Object -ExpandProperty Lines\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003EРазмер книги получился 215к токенов, не влезает ни в 110к, ни даже в максимальные 160к. В таком случае, когда фактический контекст больше размера \u003Ccode\u003E-c\u003C\u002Fcode\u003E  подключается context shift, он обрезает часть токенов и это сильно снижает точность и качество. Проверим как это работает, а потом урежем контекст книги.\u003C\u002Fp\u003E\u003Cp\u003EВ системный промпт скопирован текст всей книги, context shift включается автоматически, попросим пересказать сюжет:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1dc\u002Fa56\u002F00f\u002F1dca5600f818608b78d8224c7deb1ce3.png\" width=\"1073\" height=\"930\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1dc\u002Fa56\u002F00f\u002F1dca5600f818608b78d8224c7deb1ce3.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1dc\u002Fa56\u002F00f\u002F1dca5600f818608b78d8224c7deb1ce3.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПервое, что удивляет, это то, что настолько экстремально квантованная модель что-то отвечает на таком огромном контексте. \u003C\u002Fp\u003E\u003Cp\u003EВторое - то, она отвечает что-то связное, описание в целом пересказано верно, ключевая особенность героя передана верно, ключевой момент с Неудачником описан верно. Но видно, что данные из первой половины книги не учтены, это работа context shift.\u003C\u002Fp\u003E\u003Cp\u003EПосмотреть на то, как context shift даёт негативный эффект, можно спросив какую-то конкретную вещь из книги. Например, попросить точно процитировать эпиграф из самого начала. В ответе полностью выдуманный текст, ничего общего с оригиналом:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcce\u002Fdf4\u002Fcb7\u002Fccedf4cb7e786eb4a1506dc5656601be.png\" alt=\"context shift не позволяет точно цитировать\" title=\"context shift не позволяет точно цитировать\" width=\"1018\" height=\"502\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcce\u002Fdf4\u002Fcb7\u002Fccedf4cb7e786eb4a1506dc5656601be.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcce\u002Fdf4\u002Fcb7\u002Fccedf4cb7e786eb4a1506dc5656601be.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003Econtext shift не позволяет точно цитировать\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EУрежем книгу до 100к токенов, теперь текст полностью влезает в контекст. Зададим тот же вопрос, и да, теперь ответ правильный:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F939\u002Fece\u002F919\u002F939ece919421b269acfc548d0dfb01f9.png\" alt=\"110к, без context shift, цитата верна\" title=\"110к, без context shift, цитата верна\" width=\"1064\" height=\"619\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F939\u002Fece\u002F919\u002F939ece919421b269acfc548d0dfb01f9.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F939\u002Fece\u002F919\u002F939ece919421b269acfc548d0dfb01f9.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E110к, без context shift, цитата верна\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПроверим способность MLA и модели работать не только с началом и концом, возьмём цитату из середины и попросим модель найти откуда эта цитата, описать что происходит в этот момент и процитировать диалог целиком.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003EВ каком месте книги кто-то сказал \"Звёзды слишком яркие.\"? Процитируй весь диалог\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fddf\u002Fcb4\u002Fd49\u002Fddfcb4d49e5ed7739e5f21853eaddcb2.png\" alt=\"100к контекста, цитирование из середины книги, диалог дословно верный\" title=\"100к контекста, цитирование из середины книги, диалог дословно верный\" width=\"931\" height=\"511\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fddf\u002Fcb4\u002Fd49\u002Fddfcb4d49e5ed7739e5f21853eaddcb2.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fddf\u002Fcb4\u002Fd49\u002Fddfcb4d49e5ed7739e5f21853eaddcb2.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E100к контекста, цитирование из середины книги, диалог дословно верный\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EОписание событий правильное, цитирование диалога правильное. Ошибка только в том, что это глава 010, а не 011. Возможно модель сбивает то, что нумерация глав в книге представлена в двоичном виде. \u003C\u002Fp\u003E\u003Ch2\u003E160к контекста на одной 4090\u003C\u002Fh2\u003E\u003Cp\u003EОсталось только проверить максимальный контекст для DeepSeek R1, который составляет 160к. Контекст PP будет считаться на GPU, а новые токены ответа TG на CPU, и сколько останется памяти, догрузим GPU целыми слоями.\u003C\u002Fp\u003E\u003Cp\u003EЕсли в предыдущем эксперименте скорости были довольно комфортные, то тут скорость будет очень низкой. Это больше про посмотреть, остаётся ли ответ модели разумным на максимальном контексте в таком экстремальном квантовании.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-server -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 18 -ts 24,0 -ctk q8_0 -ctv q8_0 -b 4096 -ub 4096 -t 28 -c 163840\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003EУдалось выгрузить 18 слоев, для большей экономии включил \u003Ccode\u003E-ctv q8_0\u003C\u002Fcode\u003E, по идее это не должно сказаться на качестве, а несколько гб высвободиться. \u003C\u002Fp\u003E\u003Cp\u003EЗагрузка контекста успешно проходит до 128к и тут мы сталкиваемся с первой проблемой:\u003C\u002Fp\u003E\u003Cp\u003E```cpy.cu:573: GGML_ASSERT(ggml_nbytes(src0) &lt;= INT_MAX) failed```\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1e3\u002F18a\u002Ff12\u002F1e318af1282163f372005e30f02d1129.png\" alt=\"Логи прогрузки 160к контекста\" title=\"Логи прогрузки 160к контекста\" width=\"1235\" height=\"668\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1e3\u002F18a\u002Ff12\u002F1e318af1282163f372005e30f02d1129.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1e3\u002F18a\u002Ff12\u002F1e318af1282163f372005e30f02d1129.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EЛоги прогрузки 160к контекста\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EИз-за размера батча 4096 мы достигли и превысили лимит INT_MAX. Возможно это исправить или нет, пока не ясно, поэтому придется снизить -b -ub, что замедлит скорость PP, но на данный момент это единственный способ загрузить столько контекста. Уменьшая размер батча, высвободиться видеопамять, значит можно выгрузить больше слоев на GPU, что может немного поможет при генерации ответа:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-server -m \"DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf\" -mla 3 -fa -amb 512 -fmoe -ot exps=CPU -ngl 30 -ts 24,0 -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 -t 28 -c 163840\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003EСкорость PP под конец упала до 75 t\u002Fs, а скорость генерации составила всего 0.5 t\u002Fs:\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff04\u002F3f9\u002F613\u002Ff043f9613dcd813280abd4659e775567.png\" width=\"512\" height=\"150\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff04\u002F3f9\u002F613\u002Ff043f9613dcd813280abd4659e775567.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff04\u002F3f9\u002F613\u002Ff043f9613dcd813280abd4659e775567.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EЗато можно убедиться, что на 160к модель всё еще не утратила связь с контекстом, и сам ответ правильный в пределах этих 160к, так как развязка осталась в оставшихся 55к.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff11\u002Fa69\u002F866\u002Ff11a69866359b2e515288c5ea9fa82ae.png\" width=\"956\" height=\"545\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff11\u002Fa69\u002F866\u002Ff11a69866359b2e515288c5ea9fa82ae.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff11\u002Fa69\u002F866\u002Ff11a69866359b2e515288c5ea9fa82ae.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EЧтобы воспользоваться всем контекстом и выгрузить все слои на GPU не хватило буквально 4-5гб, поэтому эту работу оставим для 5090, а для 4090 пределом будет 80к.\u003C\u002Fp\u003E\u003Ch2\u003EБонус. Огромный контекст на LLama 4 и Gemma3\u003C\u002Fh2\u003E\u003Cp\u003EDeepSeek не единственная модель, в которую можно загрузить много контекста, но единственная у которой есть MLA. Для других моделей стандартным решением является SWA - скользящее окно внимания.\u003C\u002Fp\u003E\u003Cp\u003EНедавно в llama.cpp добавили \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fggml-org\u002Fllama.cpp\u002Fpull\u002F13194\" rel=\"noopener noreferrer nofollow\"\u003Eподдержку SWA\u003C\u002Fa\u003E, которое позволяет обрабатывать огромные последовательности требуя мало памяти. Качество должно быть ниже чем у MLA, так как уходя за пределы окна происходит очистка SWA-кэша и, после определенной фиксации в пределах окна, забывание токенов данных, но проверить всё равно можно. Этой поддержки пока нет в ik_llama, поэтому запускать надо на llama.cpp.\u003C\u002Fp\u003E\u003Cp\u003ESWA работает и для Llama 4 и для Gemma3, включается через \u003Ccode\u003E-fa\u003C\u002Fcode\u003E, есть возможность включить \u003Ccode\u003E--swa-full\u003C\u002Fcode\u003E, в этом режиме размер кэша SWA равен полному контексту, очищения SWA кэша не происходит, но памяти потребует намного больше. При использовании SWA context shift автоматически отключается.\u003C\u002Fp\u003E\u003Cp\u003EУ Llama 4 Scout (108B-A17B) размер контекста 10м, У Llama 4 Maverick (401B-A17B) - 1м. Этого хватит, чтобы вместить книгу целиком. У Gemma3 27B только 128к, но она славится тем, чем её контекст очень тяжелый, поэтому SWA должен с этим помочь.\u003C\u002Fp\u003E\u003Cp\u003EGemma3 это Dense модель, а Llama 4 это MoE, у Scout 16 экспертов, а у Maverick 128. У Llama 4 есть много общих слоев, поэтому эти модели выдают очень хорошую скорость при использовании \u003Ccode\u003E-ot\u003C\u002Fcode\u003E, а Gemma3 в целом легковесная.\u003C\u002Fp\u003E\u003Cp\u003EДля начала посмотрим сколько токенизаторы Llama 4 и Gemma3 найдут токенов у книги:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-tokenize -m \"Llama-4-Maverick-17B-128E-Instruct-UD-Q3_\u003Cbr\u002F\u003EK_XL-00001-of-00004.gguf\" -f \"\u003Cu\u003Elabir_otra.txt\"\u003C\u002Fu\u003E | wc -l\u003C\u002Fcode\u003E\u003Cbr\u002F\u003E\u003Ccode\u003E&gt; 182120\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\\llama-tokenize.exe -m \"gemma-3-27b-it-Q4_K_M.gguf\" -f \"labir_otra.txt\" | Measure-Object -Line | Select-Object -ExpandProperty Lines\u003C\u002Fcode\u003E\u003Cbr\u002F\u003E\u003Ccode\u003E&gt; 190549\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003E177к токенов у Llama4 и 186к у Gemma3. Видимо у них токенизатор лучше подходит для текста на русском, но как это скажется на качестве пока не ясно. \u003C\u002Fp\u003E\u003Cp\u003EМодели запускаем через llama.cpp, логика такая же как и раньше, только нужно убрать параметры которых нет в llama.cpp. Модели не запускаются с \u003Ccode\u003E-fa\u003C\u002Fcode\u003E, если квантование кэшей не синхронно, поэтому придётся указать и \u003Ccode\u003E-ctv q8_0\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\u003Ch4\u003ELlama 4 Maverick\u003C\u002Fh4\u003E\u003Cp\u003EМаксимально можно вместить 350к контекста в стандартных размераз ub\u002Fb и примерно 210к для \u003Ccode\u003E-ub 3072 -b 3072\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003EКвант UD-Q3_K_XL.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ccode\u003E.\u002Fllama-server -m \"Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00001-of-00004.gguf\" -fa -ctk q8_0 -ctv q8_0 -c 215040 -ot exps=CPU -ngl 99 -ts 24,0 -t 28\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\u003Cp\u003EМодель правильно собрала вместе части, но в 3 части 8 глав, а не 4. Видимо модель сбивает нумерация в бинарном виде, даже если явно ей это сказать. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fee5\u002F244\u002Ff88\u002Fee5244f88aa2731992a971c84f56f810.png\" alt=\"\" title=\"\" width=\"952\" height=\"718\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fee5\u002F244\u002Ff88\u002Fee5244f88aa2731992a971c84f56f810.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fee5\u002F244\u002Ff88\u002Fee5244f88aa2731992a971c84f56f810.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПроверим точность цитирования из середины. Цитата точная, описание не совсем точное. Глава указана правильно, а название части модель выдумала по тому, где находятся герои, правильное название \"ЧАСТЬ ВТОРАЯ. ЛАБИРИНТ\":\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F9d3\u002Ff40\u002F61c\u002F9d3f4061ca797cb9a7c5eb5b79542d36.png\" alt=\"Llama 4 Maverick, книга целиком в контексте\" title=\"Llama 4 Maverick, книга целиком в контексте\" width=\"928\" height=\"666\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F9d3\u002Ff40\u002F61c\u002F9d3f4061ca797cb9a7c5eb5b79542d36.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F9d3\u002Ff40\u002F61c\u002F9d3f4061ca797cb9a7c5eb5b79542d36.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003ELlama 4 Maverick, книга целиком в контексте\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПопробуем запутать модель, она должна понять, что пивной ларёк это на самом деле бар и упомянуть про \"Ждите отстоя пены\".\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb14\u002F5ac\u002F07d\u002Fb145ac07d81439bbef2818d9ebd04170.png\" alt=\"llama 4 maverick, ответы правильные\" title=\"llama 4 maverick, ответы правильные\" width=\"985\" height=\"909\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb14\u002F5ac\u002F07d\u002Fb145ac07d81439bbef2818d9ebd04170.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb14\u002F5ac\u002F07d\u002Fb145ac07d81439bbef2818d9ebd04170.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003Ellama 4 maverick, ответы правильные\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E С этим модель справляется успешно. То есть в пределах SWA окна у модели нет проблем.\u003C\u002Fp\u003E\u003Cp\u003EТеперь вопрос на контексте 32к, чтобы на нём же протестировать более маленькие модели.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003EВ какой момент была включена Roll Over Beethoven? Что произошло дальше?\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F78d\u002F8b0\u002F286\u002F78d8b0286ebb21cc5a7b6baf2926d74c.png\" alt=\"Llama 4 Maverick, правильный ответ для тестирования\" title=\"Llama 4 Maverick, правильный ответ для тестирования\" width=\"929\" height=\"369\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F78d\u002F8b0\u002F286\u002F78d8b0286ebb21cc5a7b6baf2926d74c.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F78d\u002F8b0\u002F286\u002F78d8b0286ebb21cc5a7b6baf2926d74c.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003ELlama 4 Maverick, правильный ответ для тестирования\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EТут модель дает правильный ответ, всё это в пределах одной главы.\u003C\u002Fp\u003E\u003Cp\u003EНебольшой тест 4060 ti, в неё влезает 70к контекста с ub\u002Fb 4096. Обработка pp очень медленная, но генерация ответа вполне быстрая. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2e9\u002Fad1\u002F877\u002F2e9ad187775e5730da04c60190d3f448.png\" alt=\"4060 ti с контекстом 70к, скорость pp низкая, скорость tg нормальная\" title=\"4060 ti с контекстом 70к, скорость pp низкая, скорость tg нормальная\" width=\"1082\" height=\"211\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2e9\u002Fad1\u002F877\u002F2e9ad187775e5730da04c60190d3f448.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2e9\u002Fad1\u002F877\u002F2e9ad187775e5730da04c60190d3f448.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E4060 ti с контекстом 70к, скорость pp низкая, скорость tg нормальная\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EОтвет правильный, так как ответ снова в пределах абзатца.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4fe\u002F5a4\u002F910\u002F4fe5a491096df8cd4699d67b05a8b473.png\" alt=\"4060 ti с контекстом 70к, модель правильно нашла место\" title=\"4060 ti с контекстом 70к, модель правильно нашла место\" width=\"911\" height=\"261\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4fe\u002F5a4\u002F910\u002F4fe5a491096df8cd4699d67b05a8b473.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4fe\u002F5a4\u002F910\u002F4fe5a491096df8cd4699d67b05a8b473.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003E4060 ti с контекстом 70к, модель правильно нашла место\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Ch4\u003ELlama 4 Scout\u003C\u002Fh4\u003E\u003Cp\u003EMaverick в целом справляется, если знать особенности SWA. Но вот у Scout с этим намного хуже, ответы хаотичные и случайные, иногда попадающие куда надо. Видимо сказывается тот факт, что у Maverick 128 экспертов, а у Scout только 16. \u003C\u002Fp\u003E\u003Cp\u003EКвант UD-Q4_K_XL. \u003C\u002Fp\u003E\u003Cp\u003EТот же вопрос в пределах 32к. Ответ не правильный по своей логике, Scout пишет, что герой уже в ресторане и там и взял диск, что не правильно, и после заказывает такси, чтобы поехать в тот самый ресторан, где он уже находится по версии модели. Видно, что модель слабее Maverick и не может удерживать события даже в своем ответе.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c2\u002F6f4\u002Fb2f\u002F2c26f4b2f83a2b97acae6e7738139707.png\" width=\"884\" height=\"363\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c2\u002F6f4\u002Fb2f\u002F2c26f4b2f83a2b97acae6e7738139707.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c2\u002F6f4\u002Fb2f\u002F2c26f4b2f83a2b97acae6e7738139707.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch4\u003EGemma3\u003C\u002Fh4\u003E\u003Cp\u003EУ Gemma3 возникает проблема на длинном контексте без квантования. Если контекст длиннее 32к, то вместо ответа получается зацикленный токен, эта проблема возникает только без квантования kv-кэша. В пределах 32к контекста ответы более менее нормальные, 32к контекст занимает 2.5гб без квантования.\u003C\u002Fp\u003E\u003Cp\u003EЕсли включить квантование \u003Ccode\u003E-ctk q8_0 -ctv q8_0\u003C\u002Fcode\u003E, то полный 128к контекст обрабатывается и занимает ~7гб, но модель слишком маленькая, всего 27B, поэтому на таком огромном контексте она показывает себя плохо.\u003C\u002Fp\u003E\u003Cp\u003EКвант Q8_0, модель не удерживает даже 32к контекста и выдумывает ответ из обрывков фраз книги, про то, что герой вернулся в реальность. Возможно, это проблема реализации SWA, но включение \u003Ccode\u003E--swa-full\u003C\u002Fcode\u003E не помогает, что говорит о том, что это проблема модели.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4e2\u002Fac1\u002F0a3\u002F4e2ac10a3313f4cf0a4a682b2cc3af90.png\" alt=\"Gemma3 27b, 32к, ответ не правильный\" title=\"Gemma3 27b, 32к, ответ не правильный\" width=\"943\" height=\"478\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4e2\u002Fac1\u002F0a3\u002F4e2ac10a3313f4cf0a4a682b2cc3af90.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4e2\u002Fac1\u002F0a3\u002F4e2ac10a3313f4cf0a4a682b2cc3af90.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EGemma3 27b, 32к, ответ не правильный\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003EЕсли не хочется возиться с консолью\u003C\u002Fh2\u003E\u003Ch4\u003Eoobabooga \u002F text-generation-webui\u003C\u002Fh4\u003E\u003Cp\u003EПока \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Foobabooga\u002Ftext-generation-webui\" rel=\"noopener noreferrer nofollow\"\u003Etext-generation-webui\u003C\u002Fa\u003E единственный клиент, где реализовали поддержку кастомных параметров командной строки, включая \u003Ccode\u003E-ot\u003C\u002Fcode\u003E. Тут нет поддержки ik_llama, но для UD-квантов DeepSeek или Llama 4 подойдет. Правда размер контекста тут ограничен 128к.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa96\u002Fc2a\u002F1c3\u002Fa96c2a1c3bdeb8cb87bb0754ba952f60.png\" width=\"1513\" height=\"961\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa96\u002Fc2a\u002F1c3\u002Fa96c2a1c3bdeb8cb87bb0754ba952f60.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa96\u002Fc2a\u002F1c3\u002Fa96c2a1c3bdeb8cb87bb0754ba952f60.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003EOpenAI API, Jan и Cherry Studio\u003C\u002Fh2\u003E\u003Cp\u003EПри запуске llama-server у ik_llama или llama.cpp создается не только веб-клиент, но и openai compatiable api, поэтому можно пользоваться любым клиентом, который умеет подключаться к openai. Два клиента с открытым кодом, который так умеют: \u003Ca href=\"https:\u002F\u002Fjan.ai\u002F\" rel=\"noopener noreferrer nofollow\"\u003EJan\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FCherryHQ\u002Fcherry-studio\" rel=\"noopener noreferrer nofollow\"\u003ECherry Studio\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cp\u003EК url веб-клиента нужно просто добавить \u002Fv1 и получить адрес api, который можно использовать в любом софте, в том числе и таком как Cline или Continue для разработки.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F941\u002F4dc\u002Ff66\u002F9414dcf66af59bff7b7f5b8071921e9b.png\" width=\"1017\" height=\"539\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F941\u002F4dc\u002Ff66\u002F9414dcf66af59bff7b7f5b8071921e9b.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F941\u002F4dc\u002Ff66\u002F9414dcf66af59bff7b7f5b8071921e9b.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003EВывод\u003C\u002Fh2\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F589\u002F306\u002F148\u002F589306148d255c0eead9849fbc0230ff.png\" alt=\"\" title=\"\" width=\"929\" height=\"473\" sizes=\"(max-width: 780px) 100vw, 50vw\" srcset=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F589\u002F306\u002F148\u002F589306148d255c0eead9849fbc0230ff.png 780w,&#10;       https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F589\u002F306\u002F148\u002F589306148d255c0eead9849fbc0230ff.png 781w\" loading=\"lazy\" decode=\"async\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EДля ускорения MoE моделей (LLama 4, DeepSeek, Qwen3) нужен параметр -ot и одна GPU, это позволит получить ощутимое ускорение\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЧтобы вместить огромный контекст в небольшой объём памяти нужно использовать MLA для DeepSeek и SWA для Llama 4 \u002F Gemma3\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E4060 ti позволит вмещать 32к контекста в DeepSeek и 70к в Llama 4 Maverick, но обрабатывает долговато, медленная память сказывается\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E4090 способна вместить 80к и обрабатывать контекст на комфортной скорости 200-300 t\u002Fs, в Maverick способна вместить 210к\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EДаже на максимальном для R1 контексте 160к мини-квант отвечает нормально\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EЭксперимент с огромным контекстом прошел лучше чем задумывался, ожидания были, что где-то уже после 8к, максимум 32к, модель совсем перестанет связно отвечать и будет много артефактов. Но не стоит ожидать, что такой маленький квант будет соответствовать качеству оригинала, хотя он и справляется лучше, чем многие другие модели.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"llama.cpp"},{"titleHtml":"ik_llama"},{"titleHtml":"deepseek"},{"titleHtml":"локальные нейросети"},{"titleHtml":"deepseek r1"},{"titleHtml":"deepseek v3"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F21c\u002F125\u002F26f\u002F21c12526f6b5a79b4350d686a03883e8.png","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F21c\u002F125\u002F26f\u002F21c12526f6b5a79b4350d686a03883e8.png","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Farticles\\\u002F921540\\\u002F\"},\"headline\":\"Запускаем настоящую DeepSeek R1 671B на игровом ПК и смотрим вменяемая ли она на огромном контексте (160к)\",\"datePublished\":\"2025-06-29T11:46:58+03:00\",\"dateModified\":\"2025-06-30T12:36:28+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Shannon\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, т...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Farticles\\\u002F921540\\\u002F#post-content-body\",\"about\":[\"h_artificial_intelligence\",\"f_popsci\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F921540\\\u002F4655f563d03dd272970ec33eb06ca4b4\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F37b\\\u002F790\\\u002F19b\\\u002F37b79019ba325f57f446bac66156b1e5.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fac2\\\u002Fb64\\\u002Fed0\\\u002Fac2b64ed0aaf4bf31a2fb9027d8003c5.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fa43\\\u002Fe07\\\u002F23e\\\u002Fa43e0723ec398948f69e9fb762c3c068.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F5ae\\\u002F612\\\u002F881\\\u002F5ae612881bf3c9c8321524e7b4ba5ce6.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F4fa\\\u002F63b\\\u002F483\\\u002F4fa63b48328fe5520c8d3e3bfcd2d762.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F50e\\\u002Fba9\\\u002Fd1a\\\u002F50eba9d1a83031a1cf402184bafb1cf1.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fbb5\\\u002F2da\\\u002Ffc0\\\u002Fbb52dafc048b82e76591d95064ed899c.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F4b0\\\u002F7e3\\\u002Ff18\\\u002F4b07e3f182c2fade910d5eb1ea8f547e.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F400\\\u002Fce3\\\u002F24f\\\u002F400ce324f0f32ea1672463ad415d2430.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F962\\\u002Fefe\\\u002Fedd\\\u002F962efeedd5c67dee42c5c6a2e46f6695.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F8a0\\\u002Fd22\\\u002F34d\\\u002F8a0d2234d5015348adf430e72241917f.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F98e\\\u002F7fa\\\u002F0f1\\\u002F98e7fa0f16d46bbff8f5c35df40c34c2.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F5e9\\\u002Fc5b\\\u002F92b\\\u002F5e9c5b92ba6f2597a01a97766de228ed.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F270\\\u002Fe54\\\u002F5e0\\\u002F270e545e068f018fbf81d72b1dca4ea7.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F488\\\u002Ff02\\\u002F787\\\u002F488f02787c0235bf8bb1d24d4a0dc810.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fea8\\\u002F4ed\\\u002F39f\\\u002Fea84ed39faacf9cf33c20e575a09d127.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F2c0\\\u002F327\\\u002Fd63\\\u002F2c0327d6306ad6e00e3501b6adea0baa.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F5de\\\u002Fc67\\\u002F19b\\\u002F5dec6719bbf3e6f84797bda523430bd8.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fe79\\\u002F021\\\u002F938\\\u002Fe7902193855bc67679ba2b46faf91439.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F7b2\\\u002F91b\\\u002F0f3\\\u002F7b291b0f377f40e1fdf0172eaf59e75d.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F461\\\u002F59b\\\u002F548\\\u002F46159b548ef07b417b78f0ad44b9c600.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Ffde\\\u002Fa2d\\\u002Fb7a\\\u002Ffdea2db7a50d436bbf2ef77e5af5eb75.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fbed\\\u002F402\\\u002Feb8\\\u002Fbed402eb8dd4d567fd34377f9db28b88.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fb55\\\u002Fbc9\\\u002Fc75\\\u002Fb55bc9c750990ad4ea0064428d09f53a.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F091\\\u002F0bc\\\u002F03c\\\u002F0910bc03ccb65ec1f3dbcae07bf5c761.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F426\\\u002Fe5b\\\u002F144\\\u002F426e5b1443fa9a83a5fcdb7883504b66.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F803\\\u002Fd8d\\\u002Fdfb\\\u002F803d8ddfb7bcf5bc71659b95da363fa9.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fe88\\\u002F845\\\u002Fad9\\\u002Fe88845ad9fb379e34bcca7c41548d68f.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F1dc\\\u002Fa56\\\u002F00f\\\u002F1dca5600f818608b78d8224c7deb1ce3.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fcce\\\u002Fdf4\\\u002Fcb7\\\u002Fccedf4cb7e786eb4a1506dc5656601be.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F939\\\u002Fece\\\u002F919\\\u002F939ece919421b269acfc548d0dfb01f9.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fddf\\\u002Fcb4\\\u002Fd49\\\u002Fddfcb4d49e5ed7739e5f21853eaddcb2.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F1e3\\\u002F18a\\\u002Ff12\\\u002F1e318af1282163f372005e30f02d1129.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Ff04\\\u002F3f9\\\u002F613\\\u002Ff043f9613dcd813280abd4659e775567.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Ff11\\\u002Fa69\\\u002F866\\\u002Ff11a69866359b2e515288c5ea9fa82ae.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fee5\\\u002F244\\\u002Ff88\\\u002Fee5244f88aa2731992a971c84f56f810.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F9d3\\\u002Ff40\\\u002F61c\\\u002F9d3f4061ca797cb9a7c5eb5b79542d36.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fb14\\\u002F5ac\\\u002F07d\\\u002Fb145ac07d81439bbef2818d9ebd04170.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F78d\\\u002F8b0\\\u002F286\\\u002F78d8b0286ebb21cc5a7b6baf2926d74c.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F2e9\\\u002Fad1\\\u002F877\\\u002F2e9ad187775e5730da04c60190d3f448.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F4fe\\\u002F5a4\\\u002F910\\\u002F4fe5a491096df8cd4699d67b05a8b473.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F2c2\\\u002F6f4\\\u002Fb2f\\\u002F2c26f4b2f83a2b97acae6e7738139707.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F4e2\\\u002Fac1\\\u002F0a3\\\u002F4e2ac10a3313f4cf0a4a682b2cc3af90.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fa96\\\u002Fc2a\\\u002F1c3\\\u002Fa96c2a1c3bdeb8cb87bb0754ba952f60.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F941\\\u002F4dc\\\u002Ff66\\\u002F9414dcf66af59bff7b7f5b8071921e9b.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F589\\\u002F306\\\u002F148\\\u002F589306148d255c0eead9849fbc0230ff.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F21c\\\u002F125\\\u002F26f\\\u002F21c12526f6b5a79b4350d686a03883e8.png\"]}","metaDescription":"Релиз DeepSeek R2 официально отложен и пока R1 не потерял актуальность, попробуем запустить модель на домашнем ПК. Оригинальная DeepSeek R1 имеет размер 700гб, так как она обучалась в fp8, но если бы...","mainImageUrl":null,"amp":true,"customTrackerLinks":[]},"polls":[],"commentsEnabled":{"status":true,"reason":null},"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"hasPinnedComments":false,"format":"tutorial","banner":null,"multiwidget":null,"multiwidgetUuid":null,"readingTime":20,"complexity":null,"isEditorial":false,"flowNew":null,"linkedPostTranslation":null}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"postReasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"viewedPosts":[],"myFeedFilter":{"complexity":"all","score":"all","types":["articles","posts","news"]},"myFeedIsApplyFilters":false,"myFeedIsForce":false},"me":{"user":null,"uuid":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null,"userUpdates":{"feeds":{"newPostsCount":null,"newThreadsCount":null,"newNewsCount":null,"newCount":null},"conversationUnreadCount":0,"trackerUnreadCount":0},"features":null},"promoData":{"isLoading":false,"hasLoaded":false,"featurer":null,"megaposts":null,"promoLinks":null,"promoPosts":null,"sticker":null,"isPromoDataAvailable":{"featurer":false,"promoPosts":true,"promoLinks":true,"megaposts":true}},"ssr":{"error":null,"isDataLoaded":true,"isDataLoading":false},"pullRefresh":{"shouldRefresh":false},"fixedBanner":{"isArticleStickyPanelVisible":false,"isArticleStickyPanelAtTheBottom":false,"isFixedBannerVisible":false},"publicationStats":{"statsInfo":{},"statsFunnels":{},"statsGraph":{},"defaultSuggest":{},"suggest":{},"timeTracker":{},"isTrackingActivity":false,"isUserActive":true,"otherPublicationStats":{}},"users":{"authorRefs":{"__ALIAS_STORE__":true},"authorIds":{},"pagesCount":{},"authorProfiles":{"__ALIAS_STORE__":true},"userHubs":{"__ALIAS_STORE__":true},"userInvitations":{"__ALIAS_STORE__":true},"authorFollowers":{"__ALIAS_STORE__":true},"authorFollowed":{"__ALIAS_STORE__":true},"userSpecialization":{"__ALIAS_STORE__":true},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"similarList":{"similarListIds":[],"similarListRefs":null},"hubs":{"hubRefs":{"__ALIAS_STORE__":true},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{"name":"","params":{},"query":{}}},"projectsBlocks":{"activeBlocks":{"questions":"project-block-article"}},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null},"stories":{"stories":null},"career":{"seoLandings":[],"hubs":"artificial_intelligence"},"events":{"eventRefs":{},"eventIds":[],"pagesCount":0,"categories":[],"cities":[],"actualEvents":null,"currentEvent":null,"eventsFilter":{"city":"all","timeStarted":null,"timeEnded":null}},"karma":{"userReasonsList":null}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="https://assets.habr.com/habr-web/js/chunk-vendors.7ab9e5e2.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.8728a42c.js" defer></script></div>
    <div id="overlays"><!----><!--teleport anchor--><!----><!--teleport anchor--><!----><!--teleport anchor--><!----><!--teleport anchor--><!----><!--teleport anchor--><!----><!--teleport anchor--></div>
    
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-S28W1WC23F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

  </script>
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
    </body>

    </html>
